{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online-Continual Few-Shot Domain Adaptation - Task-IL Method- Naive Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from subprocess import call\n",
    "import datetime\n",
    "import logging \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import json\n",
    "from dotmap import DotMap\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "from pprint import pprint\n",
    "\n",
    "import random\n",
    "import shutil\n",
    "import socket\n",
    "\n",
    "from PIL import Image\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from CL.utils import (AverageMeter, datautils, is_div, per, reverse_domain,\n",
    "                       torchutils, utils)\n",
    "\n",
    "from CL.models import (CosineClassifier, MemoryBank, SSDALossModule,\n",
    "                        compute_variance, loss_info, torch_kmeans,\n",
    "                        update_data_memory)\n",
    "from CL.utils import check_pretrain_dir, load_json, process_config, set_default\n",
    "\n",
    "#from pcs.agents import BaseAgent\n",
    "\n",
    "import json\n",
    "from dotmap import DotMap\n",
    "\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "from pcs_cl.utils import print_info, torchutils\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6.0a0+35d732a\n"
     ]
    }
   ],
   "source": [
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check CUDA status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python VERSION: 3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59) \n",
      "[GCC 7.5.0]\n",
      "pytorch VERSION: 1.5.1\n",
      "CUDA VERSION: 10.2\n",
      "CUDNN VERSION: 7605\n",
      "Device NAME: Tesla V100-PCIE-16GB\n",
      "Number CUDA Devices: 1\n",
      "Available devices: 1\n",
      "current CUDA Device: 0\n",
      "nvidia-smi:\n"
     ]
    }
   ],
   "source": [
    "def print_cuda_statistics(nvidia_smi=True, output=print):\n",
    "    output(f\"Python VERSION: {sys.version}\")\n",
    "    output(f\"pytorch VERSION: {torch.__version__}\")\n",
    "    output(f\"CUDA VERSION: {torch.version.cuda}\")\n",
    "    output(f\"CUDNN VERSION: {torch.backends.cudnn.version()}\")\n",
    "    output(f\"Device NAME: {torch.cuda.get_device_name(0)}\")\n",
    "    output(f\"Number CUDA Devices: {torch.cuda.device_count()}\")\n",
    "    output(f\"Available devices: {torch.cuda.device_count()}\")\n",
    "    output(f\"current CUDA Device: {torch.cuda.current_device()}\")\n",
    "\n",
    "    if nvidia_smi:\n",
    "        print(\"nvidia-smi:\")\n",
    "        call(\n",
    "            [\n",
    "                \"nvidia-smi\",\n",
    "                \"--format=csv\",\n",
    "                \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\",\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "\n",
    "output = print_cuda_statistics(nvidia_smi=True, output=print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct 16 13:12:29 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:2F:00.0 Off |                    0 |\r\n",
      "| N/A   46C    P0    37W / 250W |  10561MiB / 16160MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json\n",
    "\n",
    "def load_json(f_path):\n",
    "    with open(f_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "    \n",
    "def save_json(obj, f_path):\n",
    "    with open(f_path, \"w\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "        \n",
    "def process_config_path(config_path, override_dotmap=None):\n",
    "    config_json = load_json(config_path)\n",
    "    return process_config(config_json, override_dotmap=override_dotmap)        \n",
    "        \n",
    "def exist_key(k):\n",
    "    is_empty_dotmap = isinstance(k, DotMap) and len(k) == 0\n",
    "    return isinstance(k, bool) or (not is_empty_dotmap and k is not None)\n",
    "\n",
    "    \n",
    "def set_default(cur_config, name, value=None, callback=None):\n",
    "    if not exist_key(cur_config[name]):\n",
    "        if value is not None:\n",
    "            cur_config[name] = value\n",
    "        elif callback is not None:\n",
    "            assert exist_key(cur_config[callback])\n",
    "            cur_config[name] = cur_config[callback]\n",
    "        elif value is None and callback is None:\n",
    "            cur_config[name] = value\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    return cur_config[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_config(config):\n",
    "    set_default(config, \"validate_freq\", value=1)\n",
    "    set_default(config, \"copy_checkpoint_freq\", value=50)\n",
    "    set_default(config, \"debug\", value=False)\n",
    "    set_default(config, \"cuda\", value=True)\n",
    "    set_default(config, \"gpu_device\", value=None)\n",
    "    set_default(config, \"pretrained_exp_dir\", value=None)\n",
    "    set_default(config, \"agent\", value=\"CDSAgent\")\n",
    "\n",
    "    # data_params\n",
    "    set_default(config.data_params, \"aug_src\", callback=\"aug\")\n",
    "    set_default(config.data_params, \"aug_tgt\", callback=\"aug\")\n",
    "    set_default(config.data_params, \"num_workers\", value=4)\n",
    "    set_default(config.data_params, \"image_size\", value=224)\n",
    "    set_default(config.data_params, \"task_num\", value=8)\n",
    "    set_default(config.data_params, \"task\", value= True)\n",
    "\n",
    "    # model_params\n",
    "    set_default(config.model_params, \"load_weight_epoch\", value=0)\n",
    "    set_default(config.model_params, \"load_memory_bank\", value=True)\n",
    "\n",
    "    # loss_params\n",
    "    num_loss = len(config.loss_params.loss)\n",
    "    set_default(config.loss_params, \"weight\", value=[1] * num_loss)\n",
    "    set_default(config.loss_params, \"start\", value=[0] * num_loss)\n",
    "    set_default(config.loss_params, \"end\", value=[1000] * num_loss)\n",
    "    if not isinstance(config.loss_params.temp, list):\n",
    "        config.loss_params.temp = [config.loss_params.temp] * num_loss\n",
    "    assert len(config.loss_params.weight) == num_loss\n",
    "    set_default(config.loss_params, \"m\", value=0.5)\n",
    "    set_default(config.loss_params, \"T\", value=0.05)\n",
    "    set_default(config.loss_params, \"pseudo\", value=True)\n",
    "\n",
    "    # optim_params\n",
    "    set_default(config.optim_params, \"batch_size_src\", callback=\"batch_size\")\n",
    "    set_default(config.optim_params, \"batch_size_tgt\", callback=\"batch_size\")\n",
    "    set_default(config.optim_params, \"batch_size_lbd\", callback=\"batch_size_lbd\")\n",
    "    set_default(config.optim_params, \"momentum\", value=0.9)\n",
    "    set_default(config.optim_params, \"nesterov\", value=True)\n",
    "    set_default(config.optim_params, \"lr_decay_rate\", value=0.1)\n",
    "    set_default(config.optim_params, \"cls_update\", value=True)\n",
    "\n",
    "    # clustering\n",
    "    if config.loss_params.clus is not None:\n",
    "        if config.loss_params.clus.type is None:\n",
    "            config.loss_params.clus = None\n",
    "        else:\n",
    "            if not isinstance(config.loss_params.clus.type, list):\n",
    "                config.loss_params.clus.type = [config.loss_params.clus.type]\n",
    "            k = config.loss_params.clus.k\n",
    "            k_task = config.loss_params.clus.k_task\n",
    "            n_k = config.loss_params.clus.n_k\n",
    "            config.k_list = k * n_k\n",
    "            config.k_list_task = k_task * n_k\n",
    "            config.loss_params.clus.n_kmeans = len(config.k_list)\n",
    "            config.loss_params.clus.n_kmeans= len(config.k_list_task)\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]: Experiment directory is located at ./exps/experiments/officehome/Art->Clipart:naiv-221016131256\n",
      "[INFO]: Experiment directory is located at ./exps/experiments/officehome/Art->Clipart:naiv-221016131256\n",
      "[INFO]: Configurations and directories successfully set up.\n",
      "[INFO]: Configurations and directories successfully set up.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Loaded:\n",
      "{'agent': 'CDSAgent',\n",
      " 'copy_checkpoint_freq': 50,\n",
      " 'data_params': {'aug': 'aug_0',\n",
      "                 'fewshot': '6',\n",
      "                 'image_size': 224,\n",
      "                 'name': 'office_home',\n",
      "                 'num_workers': 4,\n",
      "                 'source': 'Art',\n",
      "                 'target': 'Clipart',\n",
      "                 'task': True,\n",
      "                 'task_num': 8},\n",
      " 'exp_base': './exps',\n",
      " 'exp_id': 'Art->Clipart:naiv',\n",
      " 'exp_name': 'officehome',\n",
      " 'loss_params': {'T': 0.05,\n",
      "                 'clus': {'k': [64, 128],\n",
      "                          'k_task': [8, 16],\n",
      "                          'kmeans_freq': 1,\n",
      "                          'n_k': 32,\n",
      "                          'type': ['each']},\n",
      "                 'loss': ['cls-so',\n",
      "                          'proto-src',\n",
      "                          'proto-tgt',\n",
      "                          'I2C-cross',\n",
      "                          'semi-condentmax',\n",
      "                          'semi-entmin',\n",
      "                          'tgt-condentmax',\n",
      "                          'tgt-entmin'],\n",
      "                 'm': 0.5,\n",
      "                 'pseudo': True,\n",
      "                 'temp': 0.1,\n",
      "                 'thres_src': 0.95,\n",
      "                 'thres_tgt': 0.95,\n",
      "                 'weight': [1, 1, 0.5, 1, 0.05, 0.5, 0.05, 0.5]},\n",
      " 'model_params': {'load_memory_bank': True,\n",
      "                  'load_weight': 'src-tgt',\n",
      "                  'load_weight_epoch': 5,\n",
      "                  'load_weight_thres': 30,\n",
      "                  'out_dim': 512,\n",
      "                  'version': 'pretrain-resnet50'},\n",
      " 'num_epochs': 500,\n",
      " 'optim_params': {'batch_size': 64,\n",
      "                  'batch_size_lbd': 32,\n",
      "                  'cls_update': True,\n",
      "                  'conv_lr_ratio': 0.1,\n",
      "                  'decay': True,\n",
      "                  'learning_rate': 0.01,\n",
      "                  'lr_decay_rate': 0.1,\n",
      "                  'momentum': 0.9,\n",
      "                  'nesterov': True,\n",
      "                  'patience': 4,\n",
      "                  'weight_decay': 0.0005},\n",
      " 'seed': 1337,\n",
      " 'steps_epoch': None,\n",
      " 'validate_freq': 1}\n",
      "\n",
      " *************************************** \n",
      "      Running experiment officehome\n",
      " *************************************** \n",
      "\n",
      "32\n",
      "True\n",
      "task_num 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config_json = load_json(\"./config_oh_AR_Cl.json\")\n",
    "pre_checkpoint_dir = check_pretrain_dir(config_json)    \n",
    "\n",
    "config = process_config(config_json)\n",
    "\n",
    "config = adjust_config(config)\n",
    "print(config.loss_params.clus.n_k)\n",
    "print(config.data_params.task)\n",
    "config.optim_params.batch_size_src\n",
    "config.gpu_device\n",
    "print(\"task_num\", config.data_params.task_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define number of classes per Domain/Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[range(0, 9), range(9, 17), range(17, 25), range(25, 33), range(33, 41), range(41, 49), range(49, 57), range(57, 65)]\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_image_label(image_list):\n",
    "    image_index = [x.split(\" \")[0] for x in open(image_list)]\n",
    "    label_list = np.array([int(x.split(\" \")[1].strip()) for x in open(image_list)])\n",
    "    return image_index, label_list\n",
    "\n",
    "\n",
    "\n",
    "def get_class_map(image_list):\n",
    "    class_map = {}\n",
    "    for x in open(image_list):\n",
    "        key = int(x.split(\" \")[1].strip())\n",
    "        if key not in class_map:\n",
    "            class_map[key] = x.split(\" \")[0].split(\"/\")[-2]\n",
    "    class_map = collections.OrderedDict(sorted(class_map.items()))\n",
    "    return class_map\n",
    "\n",
    "\n",
    "def get_class_num(image_list):\n",
    "    # return len(get_class_map(image_list))\n",
    "    return max(list(get_class_map(image_list).keys())) + 1\n",
    "\n",
    "\n",
    "domain_map = {\n",
    "            \"source\": \"Art\",\n",
    "            \"target\": \"Clipart\",\n",
    "        }\n",
    "\n",
    "name = config.data_params.name\n",
    "domain =domain_map\n",
    "\n",
    "\n",
    "num_class = get_class_num(\n",
    "            f'./data/splits/{name}/{domain[\"source\"]}.txt' )\n",
    "\n",
    "\n",
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n",
    "\n",
    "\n",
    "task_classes_arr = list(split(range(num_class), config.data_params.task_num))\n",
    "print(task_classes_arr)\n",
    "\n",
    "\n",
    "num_class_task = num_class // len((task_classes_arr)) \n",
    "print(num_class_task)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datautils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "datasets_path = {\n",
    "    \"office\": \"./data/office\",\n",
    "    \"office_home\": \"./data/officehome\",\n",
    "    \"visda17\": \"./data/visda17\",\n",
    "    \"domainnet\": \"./data/domainnet\",\n",
    "}\n",
    "\n",
    "domain_map = {\n",
    "            \"source\": \"Art\",\n",
    "            \"target\": \"Clipart\",\n",
    "        }\n",
    "\n",
    "\n",
    "def get_fewshot_index(lbd_dataset, whl_dataset):\n",
    "    lbd_imgs = lbd_dataset.imgs\n",
    "    whl_imgs = whl_dataset.imgs\n",
    "    fewshot_indices = [whl_imgs.index(path) for path in lbd_imgs]\n",
    "    fewshot_labels = lbd_dataset.labels\n",
    "    return fewshot_indices, fewshot_labels\n",
    "\n",
    "\n",
    "def get_class_map(image_list):\n",
    "    class_map = {}\n",
    "    for x in open(image_list):\n",
    "        key = int(x.split(\" \")[1].strip())\n",
    "        if key not in class_map:\n",
    "            class_map[key] = x.split(\" \")[0].split(\"/\")[-2]\n",
    "    class_map = collections.OrderedDict(sorted(class_map.items()))\n",
    "    return class_map\n",
    "\n",
    "\n",
    "def get_class_num(image_list):\n",
    "    # return len(get_class_map(image_list))\n",
    "    return max(list(get_class_map(image_list).keys())) + 1\n",
    "\n",
    "\n",
    "\n",
    "def get_fewshot_index(lbd_dataset, whl_dataset):\n",
    "    lbd_imgs = lbd_dataset.imgs\n",
    "    whl_imgs = whl_dataset.imgs\n",
    "    fewshot_indices = [whl_imgs.index(path) for path in lbd_imgs]\n",
    "    fewshot_labels = lbd_dataset.labels\n",
    "    return fewshot_indices, fewshot_labels\n",
    "\n",
    "\n",
    "def get_augmentation(trans_type=\"aug_0\", image_size=224):\n",
    "       # stat = \"imagenet\"\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std =  [0.229, 0.224, 0.225]\n",
    "    image_s = image_size + 32\n",
    "\n",
    "    data_transforms = {\n",
    "        \"raw\": transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((image_s, image_s)),\n",
    "                transforms.CenterCrop(image_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=mean, std=std),\n",
    "            ]\n",
    "        ),\n",
    "        \"aug_0\": transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((image_s, image_s)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomCrop(image_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=mean, std=std),\n",
    "            ]\n",
    "        ),\n",
    "        \"aug_1\": transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomResizedCrop(image_size, scale=(0.2, 1.0)),\n",
    "                transforms.RandomGrayscale(p=0.2),\n",
    "                transforms.ColorJitter(0.4, 0.4, 0.4, 0.4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=mean, std=std),\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    return data_transforms[trans_type]\n",
    "\n",
    "\n",
    "\n",
    "class Imagelists(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_list,\n",
    "        root,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        keep_in_mem=False,\n",
    "        ret_index=False,\n",
    "    ):\n",
    "        #print(image_list)\n",
    "        imgs, labels = create_image_label(image_list)\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.root = root\n",
    "        self.ret_index = ret_index\n",
    "        self.keep_in_mem = keep_in_mem\n",
    "        self.loader = pil_loader\n",
    "       \n",
    "\n",
    "        # keep in mem\n",
    "        if self.keep_in_mem:\n",
    "            images = []\n",
    "            for index in range(len(self.imgs)):\n",
    "                path = os.path.join(self.root, self.imgs[index])\n",
    "                img = self.loader(path)\n",
    "                if self.transform is not None:\n",
    "                    img = self.transform(img)\n",
    "                images.append(img)\n",
    "            self.images = images\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is\n",
    "            class_index of the target class.\n",
    "        \"\"\"\n",
    "        if self.keep_in_mem:\n",
    "            img = self.images[index]\n",
    "           \n",
    "        else:\n",
    "            path = os.path.join(self.root, self.imgs[index])\n",
    "            img = self.loader(path)\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "\n",
    "                \n",
    "        target = self.labels[index]  # target is label asocciated with an index\n",
    "        \n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "         \n",
    "\n",
    "        if not self.ret_index:\n",
    "            return img, target\n",
    "        else:\n",
    "            return index, img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "def create_dataset(\n",
    "    name,\n",
    "    domain,\n",
    "    txt=\"\",\n",
    "    suffix=\"\",\n",
    "    keep_in_mem=False,\n",
    "    ret_index=False,\n",
    "    image_transform=None,\n",
    "    use_mean_std=False,\n",
    "    image_size=224,\n",
    "):\n",
    "    if suffix != \"\":\n",
    "        suffix = \"_\" + suffix\n",
    "    if txt == \"\":\n",
    "        txt = f\"{domain}{suffix}\"\n",
    "        print(\"txt:\" , txt)\n",
    "    #stat = f\"{name}_{domain}\" if use_mean_std else \"imagenet\"\n",
    "    #if image_transform is not None and isinstance(image_transform, str):\n",
    "    transform = get_augmentation(image_transform, image_size=image_size )\n",
    "\n",
    "    return Imagelists(\n",
    "        f\"data/splits/{name}/{txt}.txt\",\n",
    "        datasets_path[name],\n",
    "        keep_in_mem=keep_in_mem,\n",
    "        ret_index=ret_index,\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def pil_loader(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert(\"RGB\")\n",
    "\n",
    "\n",
    "def worker_init_seed(worker_id):\n",
    "    np.random.seed(12 + worker_id)\n",
    "    random.seed(12 + worker_id)\n",
    "\n",
    "\n",
    "\n",
    "def create_loader(dataset, batch_size,  num_workers=4, is_train=True):\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=min(batch_size, len(dataset)),\n",
    "        #batch_size= len(dataset), \n",
    "        num_workers=num_workers,\n",
    "        shuffle=is_train,\n",
    "        drop_last=is_train,\n",
    "        pin_memory=True,\n",
    "        worker_init_fn=worker_init_seed,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaseAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "from pcs_cl.utils import print_info, torchutils\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "class BaseAgent(object):\n",
    "    \"\"\"\n",
    "    General agent class\n",
    "    Abstract Methods to be implemented:\n",
    "        _load_datasets\n",
    "        _create_model\n",
    "        _create_optimizer\n",
    "        train_one_epoch\n",
    "        validate\n",
    "        load_checkpoint\n",
    "        save_checkpoint\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        # set seed as early as possible\n",
    "        torchutils.set_seed(self.config.seed)\n",
    "\n",
    "        self.model = None\n",
    "        self.optim = None\n",
    "        self.logger = logging.getLogger(\"Agent\")\n",
    "        self.summary_writer = SummaryWriter(log_dir=self.config.summary_dir)\n",
    "        \n",
    "        \n",
    "        self.current_epoch = 0\n",
    "        self.current_iteration = 0\n",
    "        self.current_val_iteration = 0\n",
    "        self.val_acc = []\n",
    "        self.train_loss = []\n",
    "        self.lr_scheduler_list = []\n",
    "\n",
    "        print_info(self.logger.info)\n",
    "        self.starttime = datetime.datetime.now()\n",
    "        self._choose_device()\n",
    "\n",
    "        # Load Dataset\n",
    "        self._load_datasets()\n",
    "\n",
    "        self._create_model()\n",
    "        self._create_optimizer()\n",
    "\n",
    "        # we need these to decide best loss\n",
    "        self.current_loss = 0.0\n",
    "        self.current_val_metric = 0.0\n",
    "        self.best_val_metric = 0.0\n",
    "        self.best_val_epoch = 0\n",
    "        self.iter_with_no_improv = 0\n",
    "\n",
    "    def get_attr(self, domain, name):\n",
    "        return getattr(self, f\"{name}_{domain}\")\n",
    "\n",
    "    def set_attr(self, domain, name, value):\n",
    "        setattr(self, f\"{name}_{domain}\", value)\n",
    "        return self.get_attr(domain, name)\n",
    "\n",
    "    def _choose_device(self):\n",
    "        # check if use gpu\n",
    "        self.is_cuda = torch.cuda.is_available()\n",
    "        if self.is_cuda and not self.config.cuda:\n",
    "            self.logger.info(\n",
    "                \"WARNING: You have a CUDA device, so you should probably enable CUDA\"\n",
    "            )\n",
    "        self.cuda = self.is_cuda & self.config.cuda\n",
    "\n",
    "        if self.cuda:\n",
    "            self.device = torch.device(\"cuda\")\n",
    "            cudnn.benchmark = True\n",
    "\n",
    "            if self.config.gpu_device is None:\n",
    "                self.config.gpu_device = list(range(torch.cuda.device_count()))\n",
    "            elif not isinstance(self.config.gpu_device, list):\n",
    "                self.config.gpu_device = [self.config.gpu_device]\n",
    "            self.gpu_devices = self.config.gpu_device\n",
    "\n",
    "            # set device when only one gpu\n",
    "            num_gpus = len(self.gpu_devices)\n",
    "            self.multigpu = num_gpus > 1 and torch.cuda.device_count() > 1\n",
    "            if not self.multigpu:\n",
    "                torch.cuda.set_device(self.gpu_devices[0])\n",
    "\n",
    "            gpu_devices = \",\".join([str(_gpu_id) for _gpu_id in self.gpu_devices])\n",
    "            self.logger.info(f\"User specified {num_gpus} GPUs: {gpu_devices}\")\n",
    "            print(\"length gpu devices\", len(self.gpu_devices))\n",
    "            self.parallel_helper_idxs = torch.arange(len(self.gpu_devices)).to(\n",
    "                self.device\n",
    "            )\n",
    "\n",
    "            self.logger.info(\"Program will run on *****GPU-CUDA***** \")\n",
    "            torchutils.print_cuda_statistics(output=self.logger.info, nvidia_smi=False)\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            self.logger.info(\"Program will run on *****CPU*****\\n\")\n",
    "\n",
    "    def _load_datasets(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _create_model(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _create_optimizer(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        The main operator\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Train function is called in run \")\n",
    "            self.train()\n",
    "            print(\"train function is running\")\n",
    "            self.cleanup()\n",
    "            print(\"After cleanup\")\n",
    "        except KeyboardInterrupt as e:\n",
    "            self.logger.info(\"Interrupt detected. Saving data...\")\n",
    "            self.backup()\n",
    "            self.cleanup()\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            self.logger.error(e, exc_info=True)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Main training loop\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        print(\"******Main training lOOP starts***********\")\n",
    "\n",
    "#             print(\"First validation is running\")\n",
    "        for epoch in range(self.current_epoch + 1, self.config.num_epochs + 1):\n",
    "            # early stop\n",
    "            patience = self.config.optim_params.patience\n",
    "            if patience and self.iter_with_no_improv > patience:\n",
    "                self.logger.info(\n",
    "                    f\"accuracy not improved in {patience} epoches, stopped\"\n",
    "                )\n",
    "                break\n",
    "            # train\n",
    "            self.current_epoch = epoch\n",
    "            print(\"******Before training one epoch********\")\n",
    "            self.train_one_epoch()\n",
    "            \n",
    "\n",
    "            for sch in self.lr_scheduler_list:\n",
    "                sch.step()\n",
    "            # save\n",
    "            self.save_checkpoint()\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        \"\"\"\n",
    "        One epoch of training\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"\n",
    "        One cycle of model validation\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backup(self):\n",
    "        \"\"\"\n",
    "        Backs up the model upon interrupt\n",
    "        \"\"\"\n",
    "        self.summary_writer.close()\n",
    "        self.save_checkpoint(filename=\"backup.pth.tar\")\n",
    "\n",
    "    def finalise(self):\n",
    "        \"\"\"\n",
    "        Do appropriate saving after model is :finished training\n",
    "        \"\"\"\n",
    "        self.backup()\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"\n",
    "        Undo any global changes that the Agent may have made\n",
    "        \"\"\"\n",
    "        if hasattr(self, \"best_val_epoch\"):\n",
    "            self.logger.info(\n",
    "                f\"Best Val acc at {self.best_val_epoch}: {self.best_val_metric:.3}\"\n",
    "            )\n",
    "        endtime = datetime.datetime.now()\n",
    "        exe_time = endtime - self.starttime\n",
    "        self.logger.info(\n",
    "            f\"End at time: {endtime.strftime('%Y.%m.%d-%H:%M:%S')}, total time: {exe_time.seconds}s\"\n",
    "        )\n",
    "\n",
    "    def copy_checkpoint(self, filename=\"checkpoint.pth.tar\"):\n",
    "        if (\n",
    "            self.config.copy_checkpoint_freq\n",
    "            and self.current_epoch % self.config.copy_checkpoint_freq == 0\n",
    "        ):\n",
    "            self.logger.info(f\"Backup checkpoint_epoch_{self.current_epoch}.pth.tar\")\n",
    "            torchutils.copy_checkpoint(\n",
    "                filename=filename,\n",
    "                folder=self.config.checkpoint_dir,\n",
    "                copyname=f\"checkpoint_epoch_{self.current_epoch}.pth.tar\",\n",
    "            )\n",
    "            \n",
    "\n",
    "    def load_checkpoint(self, filename):\n",
    "        \"\"\"\n",
    "        Latest checkpoint loader\n",
    "        :param file_name: name of the checkpoint file\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        \n",
    "\n",
    "    def save_checkpoint(self, filename=\"checkpoint.pth.tar\"):\n",
    "        \"\"\"\n",
    "        Checkpoint saver\n",
    "        :param file_name: name of the checkpoint file\n",
    "        :param is_best: boolean flag to indicate whether current checkpoint's metric is the best so far\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDSAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ls_abbr = {\n",
    "    \"cls-so\": \"cls\",\n",
    "    \"proto-each\": \"P\",\n",
    "    \"proto-src\": \"Ps\",\n",
    "    \"proto-tgt\": \"Pt\",\n",
    "    \"cls-info\": \"info\",\n",
    "    \"I2C-cross\": \"C\",\n",
    "    \"semi-condentmax\": \"sCE\",\n",
    "    \"semi-entmin\": \"sE\",\n",
    "    \"tgt-condentmax\": \"tCE\",\n",
    "    \"tgt-entmin\": \"tE\",\n",
    "    \"ID-each\": \"I\",\n",
    "    \"CD-cross\": \"CD\",\n",
    "   }\n",
    "\n",
    "\n",
    "class Agent(BaseAgent):\n",
    "    \n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self._define_task(config)\n",
    "        self.is_features_computed = False\n",
    "        self.current_iteration_source = self.current_iteration_target = 0\n",
    "        self.domain_map = {\n",
    "            \"source\": self.config.data_params.source,\n",
    "            \"target\": self.config.data_params.target,\n",
    "            }\n",
    "        \n",
    "        self.task_classes_arr = task_classes_arr\n",
    "        print(\"task_classes_arr:\" , self.task_classes_arr)\n",
    "        \n",
    "        self.task_num = config.data_params.task_num\n",
    "        print(\"tasks_num\", self.task_num)\n",
    "          \n",
    "        super(Agent, self).__init__(config)\n",
    "        \n",
    " \n",
    "       \n",
    "        #For TASK-IL \n",
    "        self.momentum_softmax_target_task = []  \n",
    "        self.momentum_softmax_source_task = []   \n",
    "        for task_id, task_classes in enumerate(self.task_classes_arr):\n",
    "            momentumsoftmax_tgt = torchutils.MomentumSoftmax(\n",
    "               len(task_classes),  m=len(self.get_attr(task_id, \"train_loader_target_task\"))) \n",
    "\n",
    "            self.momentum_softmax_target_task.append(momentumsoftmax_tgt)\n",
    "            \n",
    "            momentumsoftmax_src = torchutils.MomentumSoftmax(\n",
    "                len(task_classes),  m=len(self.get_attr(task_id, \"train_loader_source_task\")))\n",
    "            \n",
    "            self.momentum_softmax_source_task.append(momentumsoftmax_src)\n",
    "\n",
    "        \n",
    "      \n",
    "        # init loss  \n",
    "        loss_fn = SSDALossModule(self.config, gpu_devices=self.gpu_devices)  \n",
    "        loss_fn = nn.DataParallel(loss_fn, device_ids=self.gpu_devices).cuda()\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "        if self.config.pretrained_exp_dir is None: \n",
    "            self._init_memory_bank()   \n",
    "\n",
    "        # init statics    \n",
    "        self._init_labels_task()\n",
    "        self._load_fewshot_to_cls_weight()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def _define_task(self, config):   \n",
    "        # specify task\n",
    "        self.fewshot = config.data_params.fewshot\n",
    "        self.task = config.data_params.task\n",
    "        self.clus = config.loss_params.clus != None\n",
    "        self.cls = self.semi = self.tgt = self.ssl = False\n",
    "        self.is_pseudo_src = self.is_pseudo_tgt = False\n",
    "        for ls in config.loss_params.loss:\n",
    "            self.cls = self.cls | ls.startswith(\"cls\")\n",
    "            self.semi = self.semi | ls.startswith(\"semi\")\n",
    "            self.tgt = self.tgt | ls.startswith(\"tgt\")\n",
    "            self.ssl = self.ssl | (ls.split(\"-\")[0] not in [\"cls\", \"semi\", \"tgt\"])\n",
    "            self.is_pseudo_src = self.is_pseudo_src | ls.startswith(\"semi-pseudo\")\n",
    "            self.is_pseudo_tgt = self.is_pseudo_tgt | ls.startswith(\"tgt-pseudo\")\n",
    "\n",
    "        self.is_pseudo_src = self.is_pseudo_src | (\n",
    "            config.loss_params.pseudo and self.fewshot is not None \n",
    "        )\n",
    "        self.is_pseudo_tgt = self.is_pseudo_tgt | config.loss_params.pseudo\n",
    "        self.semi = self.semi | self.is_pseudo_src\n",
    "        if self.clus:\n",
    "            self.is_pseudo_tgt = self.is_pseudo_tgt | (\n",
    "                config.loss_params.clus.tgt_GC ==  \"PGC\" and \"GC\" in config.clus.type \n",
    "            )\n",
    "    \n",
    "    \n",
    "  \n",
    "    #.........................Modified_2\n",
    "    def _init_labels_task(self): #return an array whose elemts other than -1 is the label of the fewshot data\n",
    "\n",
    "        for task_id, _ in enumerate(self.task_classes_arr): \n",
    "            self.task_id = task_id\n",
    "            train_len_tgt_tsk = self.get_attr(self.task_id, \"train_len_init_target_task\")\n",
    "            train_len_src_tsk = self.get_attr(self.task_id, \"train_len_init_source_task\")\n",
    "\n",
    "            # labels for pseudo\n",
    "            if self.fewshot:\n",
    "                self.predict_ordered_labels_pseudo_source_tsk = (\n",
    "                    torch.zeros(train_len_src_tsk, dtype=torch.long).detach().cuda() - 1 #creates a tensor of -1's \n",
    "                )\n",
    "\n",
    "\n",
    "                for ind, lbl in zip(self.fewshot_index_source_tasks[self.task_id,:], self.fewshot_label_source_tasks[self.task_id,:]):\n",
    "\n",
    "                    self.predict_ordered_labels_pseudo_source_tsk[int(ind)] = int(lbl) #change the -1's to lbl's only for those in self.fewshot_index_source\n",
    "\n",
    "            self.predict_ordered_labels_pseudo_target_tsk = (\n",
    "                torch.zeros(train_len_tgt_tsk, dtype=torch.long).detach().cuda() - 1\n",
    "            )\n",
    "\n",
    "            self.set_attr(self.task_id, \"predict_ordered_labels_pseudo_source_tsk\", self.predict_ordered_labels_pseudo_source_tsk) \n",
    "            self.set_attr(self.task_id, \"predict_ordered_labels_pseudo_target_tsk\", self.predict_ordered_labels_pseudo_target_tsk) \n",
    " \n",
    "        \n",
    "\n",
    "    #...............................\n",
    "    def _load_datasets(self):\n",
    "\n",
    "        name = self.config.data_params.name\n",
    "        num_workers = self.config.data_params.num_workers\n",
    "        fewshot = self.config.data_params.fewshot\n",
    "        domain = self.domain_map\n",
    "        image_size = self.config.data_params.image_size\n",
    "        aug_src = self.config.data_params.aug_src\n",
    "        aug_tgt = self.config.data_params.aug_tgt\n",
    "        raw = \"raw\"\n",
    "\n",
    "        \n",
    "        #..................................\n",
    "        self.num_class = get_class_num(\n",
    "            f'./data/splits/{name}/{domain[\"source\"]}.txt'\n",
    "        )\n",
    "\n",
    "\n",
    "        self.class_map = get_class_map(\n",
    "            f'data/splits/{name}/{domain[\"target\"]}.txt'\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "        self.num_class_task = self.num_class // self.task_num \n",
    "        print(\"num_class_task:\" , self.num_class_task)\n",
    "        \n",
    "        #....................................\n",
    "        batch_size_dict = {\n",
    "            \"test\": self.config.optim_params.batch_size,\n",
    "            \"source\": self.config.optim_params.batch_size_src,\n",
    "            \"target\": self.config.optim_params.batch_size_tgt,\n",
    "            \"labeled\": self.config.optim_params.batch_size_lbd,\n",
    "        }\n",
    "        self.batch_size_dict = batch_size_dict\n",
    " \n",
    "\n",
    "        #............................self-supervised Dataset.......\n",
    "    \n",
    "        for domain_name in (\"source\", \"target\"):\n",
    "            aug_name = {\"source\": aug_src, \"target\": aug_tgt}[domain_name]\n",
    "\n",
    "            #print(\"domain_name: \", domain_name) \n",
    "\n",
    "            # Training datasets\n",
    "            train_dataset = create_dataset(\n",
    "                name,\n",
    "                domain[domain_name],\n",
    "                suffix=\"\",\n",
    "                ret_index=True,\n",
    "                image_transform=aug_name,\n",
    "                use_mean_std=False,\n",
    "                image_size=image_size,)\n",
    "\n",
    "            train_loader = create_loader(\n",
    "                train_dataset,\n",
    "                batch_size_dict[domain_name],\n",
    "                is_train=True,\n",
    "                num_workers=num_workers,\n",
    "            )\n",
    "            train_init_loader = create_loader(          \n",
    "                train_dataset,\n",
    "                batch_size_dict[domain_name],\n",
    "                is_train=False,\n",
    "                num_workers=num_workers,\n",
    "            )\n",
    "            \n",
    "    \n",
    "            train_labels = torch.from_numpy(train_dataset.labels).detach().cuda()\n",
    "            print(\"debug-train-labels-shape:\", train_labels.shape)\n",
    "            \n",
    "            self.set_attr(domain_name, \"train_dataset\", train_dataset)  #setattr(domain, name, value) --> train_dataset_source/train_dataset_target\n",
    "            self.set_attr(domain_name, \"train_ordered_labels\", train_labels)\n",
    "            self.set_attr(domain_name, \"train_loader\", train_loader)\n",
    "            self.set_attr(domain_name, \"train_init_loader\", train_init_loader)\n",
    "            self.set_attr(domain_name, \"train_len\", len(train_dataset))    \n",
    "            \n",
    "           \n",
    "            #.........................train_task\n",
    "            print(\".......Task IL starts......\")\n",
    "            \n",
    "            #task_classes_arr = [(0,1,2,3,4), (5,6,7,8,9), (10,11,12,13,14), (15,16,17,18,19), (20,21,22,23,24), (25,26,27,28,29) ] #,30\n",
    "            #task_classes_arr = [tuple(list(range(i*tasks_num,((i+1)*tasks_num)+2))) for i in range(0,int(len(np.unique(labels))/tasks_num))]  #with overlap and without overlap\n",
    "            #tasks_num = len(task_classes_arr)\n",
    "\n",
    "            \n",
    "            #********************************train-dataset-Source\n",
    "            if domain_name == \"source\":\n",
    "                \n",
    "                train_dataset_source =  self.get_attr(\"source\", \"train_dataset\") \n",
    "                train_loader_source = self.get_attr(\"source\",\"train_loader\")\n",
    "                train_init_loader_source = self.get_attr(\"source\",\"train_init_loader\")\n",
    "                \n",
    "                #.................train-dataset-not-drop_lost-source\n",
    "                x_train_source_idx = torch.Tensor([0])#.cuda()\n",
    "                x_train_source_img = torch.zeros((1,3,224,224))#.cuda()\n",
    "                x_train_source_lbl = torch.Tensor([0])#.cuda()\n",
    "\n",
    "                for idx , data in enumerate(train_loader_source):\n",
    "\n",
    "                    indx, img, label = data\n",
    "#                     indx = indx.cuda()\n",
    "#                     img = img.cuda()\n",
    "#                     label = label.cuda()\n",
    "\n",
    "                    x_train_source_idx=x_train_source_idx.to(torch.float32)\n",
    "                    indx=indx.to(torch.float32)\n",
    "                    label=label.to(torch.float32)\n",
    "                    x_train_source_idx = torch.cat((x_train_source_idx, indx), 0)\n",
    "                    x_train_source_img = torch.cat((x_train_source_img, img), 0)\n",
    "                    x_train_source_lbl = torch.cat((x_train_source_lbl, label), 0)\n",
    "\n",
    "                image = x_train_source_img[1:]     \n",
    "                label = x_train_source_lbl[1:]\n",
    "                index = x_train_source_idx[1:]\n",
    "\n",
    "                # Divide the data over the different tasks\n",
    "                task_data_source = [] \n",
    "                for task_id, task_classes in enumerate(self.task_classes_arr):\n",
    "                    \n",
    "                    \n",
    "                    train_mask = np.isin(label, task_classes)\n",
    "                    idx_train_task, x_train_task, y_train_task = index[train_mask], image[train_mask], label[train_mask]\n",
    "                    task_data_source.append((idx_train_task, x_train_task, y_train_task))  #- (task_id * 2) --> with overlap\n",
    "           \n",
    "\n",
    "            \n",
    "            \n",
    "                #create tensor dataset\n",
    "                for task_id, _ in enumerate(self.task_classes_arr):\n",
    "                    dataset_source_task = TensorDataset(task_data_source[task_id][0], task_data_source[task_id][1], task_data_source[task_id][2])\n",
    "                    dataloader_source_task = DataLoader(dataset_source_task, batch_size= batch_size_dict[domain_name]) \n",
    "                    \n",
    "\n",
    "                    train_labels_source_task = task_data_source[task_id][2] #dataset_source_task.labels.detach() #.cuda() #torch.from_numpy(dataset_source_task.labels).detach().cuda()\n",
    "\n",
    "        \n",
    "                    self.set_attr(task_id, \"train_data_source_task\", dataset_source_task)  \n",
    "                    self.set_attr(task_id, \"train_loader_source_task\", dataloader_source_task)  \n",
    "                    \n",
    "                    train_data_source_task = self.get_attr(task_id, \"train_data_source_task\")\n",
    "                    self.set_attr(task_id, \"train_len_source_task\", len(train_data_source_task))  \n",
    "                    self.set_attr(task_id, \"train_ordered_labels_source_task\", train_labels_source_task)\n",
    "               \n",
    "                \n",
    "                \n",
    "                #*****************train-init-dataset-Source-not-drop-last-unfulled-batch\n",
    "                \n",
    "                x_train_init_source_idx = torch.tensor([0])\n",
    "                x_train_init_source_img = torch.zeros((1,3,224,224))\n",
    "                x_train_init_source_lbl = torch.tensor([0])\n",
    "\n",
    "                for idx , data in enumerate(train_init_loader_source):\n",
    "\n",
    "                    indx, img, label = data\n",
    "#                     print(indx.shape)\n",
    "        \n",
    "                    x_train_init_source_idx = torch.cat((x_train_init_source_idx, indx), 0)\n",
    "                    x_train_init_source_img = torch.cat((x_train_init_source_img, img), 0)\n",
    "                    x_train_init_source_lbl = torch.cat((x_train_init_source_lbl, label), 0)\n",
    "\n",
    "                image = x_train_init_source_img[1:]     \n",
    "                label = x_train_init_source_lbl[1:]\n",
    "                index = x_train_init_source_idx[1:]\n",
    "\n",
    "                # Divide the data over the different tasks\n",
    "                task_data_init_source = []   \n",
    "                for task_id, task_classes in enumerate(self.task_classes_arr):\n",
    "\n",
    "                    train_mask = np.isin(label, task_classes)\n",
    "                    print(train_mask)\n",
    "                    idx_train_task, x_train_task, y_train_task = index[train_mask], image[train_mask], label[train_mask]\n",
    "                    task_data_init_source.append((idx_train_task, x_train_task, y_train_task))  \n",
    "                   \n",
    "              \n",
    "\n",
    "                #create tensor dataset\n",
    "                for task_id, _ in enumerate(self.task_classes_arr):\n",
    "                    dataset_init_source_task = TensorDataset(task_data_init_source[task_id][0], task_data_init_source[task_id][1], task_data_init_source[task_id][2])\n",
    "                    dataloader_init_source_task = DataLoader(dataset_init_source_task, batch_size= batch_size_dict[domain_name]) # create your dataloader\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    self.set_attr(task_id, \"train_data_init_source_task\", dataset_init_source_task)  \n",
    "                    self.set_attr(task_id, \"train_loader_init_source_task\", dataloader_init_source_task)  \n",
    "                    train_data_init_source_task = self.get_attr(task_id, \"train_data_init_source_task\")\n",
    "                    self.set_attr(task_id, \"train_len_init_source_task\", len(train_data_init_source_task)) \n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            #...............................Target\n",
    "            print(\"*******Create tarin-target-task************\")\n",
    "            \n",
    "            if domain_name == \"target\":\n",
    "                \n",
    "                \n",
    "                train_dataset_target =  self.get_attr(\"target\", \"train_dataset\")   \n",
    "                train_loader_target = self.get_attr(\"target\", \"train_loader\") \n",
    "                train_init_loader_target = self.get_attr(\"target\",\"train_init_loader\")\n",
    "                \n",
    "                \n",
    "            \n",
    "                x_train_target_idx = torch.tensor([0])#.cuda()\n",
    "                x_train_target_img = torch.zeros((1,3,224,224))#.cuda()\n",
    "                x_train_target_lbl = torch.tensor([0])#.cuda()\n",
    "\n",
    "                for idx , data in enumerate(train_loader_target):\n",
    "\n",
    "                    indx, img, label = data\n",
    "#                     indx = indx.cuda()\n",
    "#                     img = img.cuda()\n",
    "#                     label = label.cuda()\n",
    "                    \n",
    "#                     x_train_target_idx=x_train_target_idx.to(torch.float32)\n",
    "#                     indx=indx.to(torch.float32)\n",
    "#                     label=label.to(torch.float32)\n",
    "                \n",
    "                    x_train_target_idx = torch.cat((x_train_target_idx, indx), 0)\n",
    "                    x_train_target_img = torch.cat((x_train_target_img, img), 0)\n",
    "                    x_train_target_lbl = torch.cat((x_train_target_lbl, label), 0)\n",
    "\n",
    "\n",
    "                image = x_train_target_img[1:]\n",
    "                label = x_train_target_lbl[1:]\n",
    "                index = x_train_target_idx[1:]\n",
    "\n",
    "                # Divide the data over the different tasks\n",
    "                task_data_target = []\n",
    "                for task_id, task_classes in enumerate(self.task_classes_arr):\n",
    "\n",
    "                    train_mask = np.isin(label, task_classes)\n",
    "                    idx_train_task, x_train_task, y_train_task = index[train_mask], image[train_mask], label[train_mask]\n",
    "                    task_data_target.append((idx_train_task,x_train_task, y_train_task))  #- (task_id * 2) #with overlap\n",
    "                    #print(\"length of train_task:\" ,len(x_train_task) ) \n",
    "\n",
    "                    \n",
    "                for task_id, _ in enumerate(self.task_classes_arr):\n",
    "        \n",
    "                    dataset_target_task = TensorDataset(task_data_target[task_id][0], task_data_target[task_id][1], task_data_target[task_id][2])\n",
    "                    dataloader_target_task = DataLoader(dataset_target_task, batch_size= batch_size_dict[domain_name])\n",
    "                    \n",
    "                    #train_labels_target_task = torch.from_numpy(dataset_target_task.labels).detach().cuda()\n",
    "                    train_labels_target_task = task_data_target[task_id][2]\n",
    "                    \n",
    "                    \n",
    "                  \n",
    "                    self.set_attr(task_id, \"train_data_target_task\", dataset_target_task)  \n",
    "                    self.set_attr(task_id, \"train_loader_target_task\", dataloader_target_task)  \n",
    "                    train_data_target_task = self.get_attr(task_id, \"train_data_target_task\")\n",
    "                    self.set_attr(task_id, \"train_len_target_task\", len(train_data_target_task))\n",
    "                    self.set_attr(task_id, \"train_ordered_labels_target_task\", train_labels_target_task)\n",
    "                  \n",
    "                    \n",
    "                train_len_target_task = self.get_attr(0, \"train_len_target_task\")   \n",
    "                print(\"*****train_len_target_task******\", train_len_target_task)\n",
    "                       \n",
    "                #*******************************train-init-dataset-target-not-drop_last-unfulled_batch\n",
    "                print(\"Create train-init-target-dataset\")\n",
    "                x_train_init_target_idx = torch.tensor([0])#.cuda()   \n",
    "                x_train_init_target_img = torch.zeros((1,3,224,224))#.cuda()\n",
    "                x_train_init_target_lbl = torch.tensor([0])#.cuda()\n",
    "\n",
    "                for idx , data in enumerate(train_init_loader_target):\n",
    "\n",
    "                    indx, img, label = data\n",
    "#                     indx = indx.cuda()\n",
    "#                     img = img.cuda()\n",
    "#                     label = label.cuda()\n",
    "    \n",
    "                    x_train_init_target_idx = torch.cat((x_train_init_target_idx, indx), 0)\n",
    "                    x_train_init_target_img = torch.cat((x_train_init_target_img, img), 0)\n",
    "                    x_train_init_target_lbl = torch.cat((x_train_init_target_lbl, label), 0)\n",
    "\n",
    "                image = x_train_init_target_img[1:]     \n",
    "                label = x_train_init_target_lbl[1:]\n",
    "                index = x_train_init_target_idx[1:]\n",
    "\n",
    "                # Divide the data over the different tasks\n",
    "                task_data_init_target = []   \n",
    "                for task_id, task_classes in enumerate(self.task_classes_arr):\n",
    "\n",
    "                    train_mask = np.isin(label.cpu(), task_classes)\n",
    "                    idx_train_task, x_train_task, y_train_task = index[train_mask], image[train_mask], label[train_mask]\n",
    "                    task_data_init_target.append((idx_train_task, x_train_task, y_train_task))  \n",
    "                   \n",
    "              \n",
    "\n",
    "                #create tensor dataset\n",
    "                for task_id, _ in enumerate(self.task_classes_arr):\n",
    "                    dataset_init_target_task = TensorDataset(task_data_init_target[task_id][0], task_data_init_target[task_id][1], task_data_init_target[task_id][2])\n",
    "                    dataloader_init_target_task = DataLoader(dataset_init_target_task, batch_size= batch_size_dict[domain_name]) # create your dataloader\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    self.set_attr(task_id, \"train_data_init_target_task\", dataset_init_target_task)  \n",
    "                    self.set_attr(task_id, \"train_loader_init_target_task\", dataloader_init_target_task)  \n",
    "                    train_data_init_target_task = self.get_attr(task_id, \"train_data_init_target_task\")\n",
    "                    self.set_attr(task_id, \"train_len_init_target_task\", len(train_data_init_target_task))  \n",
    "                \n",
    "                \n",
    "                train_len_init_target_task = self.get_attr(0, \"train_len_init_target_task\")   \n",
    "                print(\"*****train_len_init_target_task******\",train_len_init_target_task)\n",
    "                       \n",
    "\n",
    "        #...............................Classification and Fewshot Dataset\n",
    "        if fewshot:\n",
    "#             print(int(fewshot))\n",
    "            print(\"**********Create few-shot dataset********\")\n",
    "            train_lbd_dataset_source = create_dataset(  #Retrun an imagelist .txt file where the labeled samples are defined\n",
    "                name,\n",
    "                domain[\"source\"],\n",
    "                suffix=f\"labeled_{fewshot}\",\n",
    "                ret_index=True,\n",
    "                image_transform=aug_src,\n",
    "                image_size=image_size,\n",
    "            )\n",
    "            \n",
    "            self.train_lbd_loader_source = create_loader(           \n",
    "                train_lbd_dataset_source,\n",
    "                batch_size_dict[\"labeled\"],\n",
    "                is_train=False,\n",
    "                num_workers=num_workers,\n",
    "            )\n",
    "             \n",
    "           \n",
    "            src_dataset = self.get_attr(\"source\", \"train_dataset\")\n",
    "            \n",
    "            (\n",
    "                self.fewshot_index_source,\n",
    "                self.fewshot_label_source,\n",
    "            ) = get_fewshot_index(train_lbd_dataset_source, src_dataset)\n",
    "            \n",
    " \n",
    "\n",
    "            test_unl_dataset_source = create_dataset(\n",
    "                name,\n",
    "                domain[\"source\"],\n",
    "                suffix=f\"unlabeled_{fewshot}\",\n",
    "                ret_index=True,\n",
    "                image_transform=raw,\n",
    "                image_size=image_size,\n",
    "            )\n",
    "            self.test_unl_loader_source = create_loader(             \n",
    "                test_unl_dataset_source,\n",
    "                batch_size_dict[\"test\"],\n",
    "                is_train=False,\n",
    "                num_workers=num_workers,\n",
    "            )\n",
    "            \n",
    "\n",
    "            #........................ task_devision -fewshot\n",
    "   \n",
    "            x_train_lbd_idx = torch.tensor([0])\n",
    "            x_train_lbd_img = torch.zeros((1,3,224,224))\n",
    "            x_train_lbd_lbl = torch.tensor([0])\n",
    "\n",
    "            for idx , data in enumerate(self.train_lbd_loader_source):   \n",
    "\n",
    "                indx, img, label = data\n",
    "\n",
    "                print(\"idx\", idx)\n",
    "                \n",
    "                print(\"index_fewshot_totall:\" , indx)\n",
    "                x_train_lbd_idx = torch.cat((x_train_lbd_idx, indx), 0)\n",
    "                x_train_lbd_img = torch.cat((x_train_lbd_img, img), 0)\n",
    "                x_train_lbd_lbl = torch.cat((x_train_lbd_lbl, label), 0)\n",
    "\n",
    "\n",
    "            image = x_train_lbd_img[1:]\n",
    "            label = x_train_lbd_lbl[1:]\n",
    "            index = x_train_lbd_idx[1:]\n",
    "\n",
    "            # Divide the data over the different tasks\n",
    "            task_train_lbd = []\n",
    "            for task_id, task_classes in enumerate(self.task_classes_arr):\n",
    "\n",
    "                train_mask = np.isin(label.cpu(), task_classes)\n",
    "                idx_train_task, x_train_task, y_train_task = index[train_mask] , image[train_mask], label[train_mask]\n",
    "                task_train_lbd.append(( idx_train_task, x_train_task, y_train_task))  #- (task_id * 2) #with overlap\n",
    "\n",
    "      \n",
    "            #...............................create dataset\n",
    "            for task_id, _ in enumerate(self.task_classes_arr):\n",
    "                dataset_task_train_lbd_source = TensorDataset(task_train_lbd[task_id][0], task_train_lbd[task_id][1],task_train_lbd[task_id][2])\n",
    "                dataloader_task_train_lbd_source = DataLoader(dataset_task_train_lbd_source, batch_size= batch_size_dict[\"test\"]) \n",
    "\n",
    "                \n",
    "                self.set_attr(task_id, \"train_data_lbd_source_task\", dataset_task_train_lbd_source) \n",
    "                self.set_attr(task_id, \"train_lbd_loader_source_task\", dataloader_task_train_lbd_source)  \n",
    "                \n",
    "                \n",
    "               \n",
    "            \n",
    "             \n",
    "            #.............colculate fewshot_index and fewshot_labels\n",
    "        \n",
    "           \n",
    "            num_labelled_each_task =  self.num_class_task          \n",
    "            self.fewshot_index_source_tasks = np.zeros((self.task_num, num_labelled_each_task*int(fewshot)))\n",
    "            self.fewshot_label_source_tasks = np.zeros((self.task_num, num_labelled_each_task*int(fewshot)))\n",
    "            length_task = np.zeros(self.task_num)\n",
    "            \n",
    "            for task_id, task_classes in enumerate(self.task_classes_arr):\n",
    "                self.task_id = task_id\n",
    "             \n",
    "                length_task[self.task_id] = self.get_attr(self.task_id, \"train_len_init_source_task\") \n",
    "                print(length_task[self.task_id]) \n",
    "                \n",
    "                if self.task_id == 0:\n",
    "                    self.fewshot_index_source_tasks[self.task_id,:] = np.array(self.fewshot_index_source)[np.isin(self.fewshot_label_source, task_classes)]\n",
    "                    self.fewshot_label_source_tasks[self.task_id,:] = np.array(self.fewshot_label_source)[np.isin(self.fewshot_label_source, task_classes)]\n",
    "                    \n",
    "                   \n",
    "                    \n",
    "                else:\n",
    "                    self.fewshot_index_source_tasks[self.task_id,:] = np.array(self.fewshot_index_source)[np.isin(self.fewshot_label_source, task_classes)] - np.sum(length_task[:self.task_id])\n",
    "                    self.fewshot_label_source_tasks[self.task_id,:] = np.array(self.fewshot_label_source)[np.isin(self.fewshot_label_source, task_classes)]\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                self.set_attr(self.task_id, \"fewshot_index_src_task\", self.fewshot_index_source_tasks[self.task_id,:])\n",
    "                self.set_attr(self.task_id, \"fewshot_label_src_task\", self.fewshot_label_source_tasks[self.task_id,:]) \n",
    "          \n",
    "             \n",
    "            \n",
    "        \n",
    "            fewshot_index_src_task_0 = self.get_attr(0, \"fewshot_index_src_task\" )\n",
    "            #print(\"fewshot_index_src_task_0 \", fewshot_index_src_task_0) \n",
    "            \n",
    "            fewshot_index_src_task_1 = self.get_attr(1, \"fewshot_index_src_task\" )\n",
    "            #print(\"fewshot_index_src_task_1 \", fewshot_index_src_task_1) \n",
    "            \n",
    "\n",
    "            fewshot_index_src_task_2 = self.get_attr(2, \"fewshot_index_src_task\" )\n",
    "            #print(\"fewshot_index_src_task_2 \", fewshot_index_src_task_2) \n",
    "                \n",
    "            \n",
    "\n",
    "            \n",
    "                 \n",
    "            # labels for fewshot\n",
    "            train_len_src_tsk = self.get_attr(task_id, \"train_len_init_source_task\")\n",
    "            #print(\"task_id\", task_id)\n",
    "            \n",
    "            fewshot_labels_tsk = (\n",
    "                torch.zeros(train_len_src_tsk, dtype=torch.long).detach().cuda() - 1\n",
    "            )\n",
    "            for ind, lbl in zip(self.fewshot_index_source_tasks[task_id ,:], self.fewshot_label_source_tasks[task_id,:]): \n",
    "\n",
    "                fewshot_labels_tsk[int(ind)] = int(lbl)\n",
    "               \n",
    "\n",
    "                        \n",
    "            #.............. Test unlabled task loader source in case fewshot                      \n",
    "                         \n",
    "            x_test_source_idx = torch.tensor([0])  \n",
    "            x_test_source_img = torch.zeros((1,3,224,224))\n",
    "            x_test_source_lbl = torch.tensor([0]\n",
    "\n",
    "            for idx , data in enumerate(self.test_unl_loader_source):\n",
    "                indx, img, label = data\n",
    "\n",
    "                x_test_source_idx = torch.cat((x_test_source_idx, indx), 0)\n",
    "                x_test_source_img = torch.cat((x_test_source_img, img), 0)\n",
    "                x_test_source_lbl = torch.cat((x_test_source_lbl, label), 0)\n",
    "\n",
    "            image = x_test_source_img[1:]     \n",
    "            label = x_test_source_lbl[1:]\n",
    "            index = x_test_source_idx[1:]\n",
    "\n",
    "            # Divide the data over the different tasks\n",
    "            task_testdata_source = []  \n",
    "            for task_id, task_classes in enumerate(self.task_classes_arr):\n",
    "                self.task_id = task_id\n",
    "                test_mask = np.isin(label.cpu(), task_classes)\n",
    "                idx_test_task, x_test_task, y_test_task = index[test_mask], image[test_mask], label[test_mask]\n",
    "                task_testdata_source.append((idx_test_task, x_test_task, y_test_task))  #- (task_id * 2) #with overlap\n",
    "                  \n",
    "           \n",
    "             #create tensor dataset\n",
    "            for task_id, _ in enumerate(self.task_classes_arr):\n",
    "                self.task_id = task_id\n",
    "                test_unl_data_source_task = TensorDataset(task_testdata_source[self.task_id][0], task_testdata_source[self.task_id][1], task_testdata_source[self.task_id][2])\n",
    "                test_unl_loader_source_task = DataLoader(test_unl_data_source_task, batch_size= batch_size_dict[\"test\"]) \n",
    "\n",
    "                \n",
    "\n",
    "                self.set_attr(self.task_id, \"test_data_source_task\", test_unl_data_source_task)  \n",
    "                self.set_attr(self.task_id, \"test_unl_loader_source_task\", test_unl_loader_source_task) \n",
    "                test_data_source_task = self.get_attr(task_id, \"test_data_source_task\")\n",
    "                self.set_attr(task_id, \"test_len_data_source_task\", len(test_data_source_task))\n",
    "           \n",
    "            \n",
    "        \n",
    "        #.........if not fewshot         \n",
    "        else:\n",
    "            print(\"******ELSE******\")\n",
    "            train_lbd_dataset_source = create_dataset(\n",
    "                name,\n",
    "                domain[\"source\"],\n",
    "                ret_index= True,\n",
    "                image_transform = aug_src,\n",
    "                image_size= image_size,\n",
    "            )\n",
    "                                  \n",
    "                                  \n",
    "            train_lbd_loader_source = create_loader(\n",
    "                train_lbd_dataset_source,\n",
    "                batch_size_dict[\"labeled\"],\n",
    "                num_workers=num_workers,\n",
    "            )\n",
    "            \n",
    "            \n",
    "            #.........train_lbd_dataset_source_task _incase_no_fewshot.........                  \n",
    "                                  \n",
    "            x_train_source_lbd_idx = torch.tensor([0]).cuda() \n",
    "            x_train_source_lbd_img = torch.zeros((1,3,224,224)).cuda()\n",
    "            x_train_source_lbd_lbl = torch.tensor([0]).cuda()\n",
    "            \n",
    "            for idx , data in enumerate(train_lbd_loader_source):\n",
    "\n",
    "                indx, img, label = data\n",
    "                indx = indx.cuda()\n",
    "                img = img.cuda()\n",
    "                label = label.cuda()\n",
    "\n",
    "                x_train_source_lbd_idx = torch.cat((x_train_source_lbd_idx, indx), 0)\n",
    "                x_train_source_lbd_img = torch.cat(( x_train_source_lbd_img , img), 0)\n",
    "                x_train_source_lbd_lbl = torch.cat((x_train_source_lbd_lbl, label), 0)\n",
    "\n",
    "            image = x_train_source_lbd_img[1:]     \n",
    "            label = x_train_source_lbd_lbl[1:]\n",
    "            index = x_train_source_lbd_idx[1:]\n",
    "\n",
    "            # Divide the data over the different tasks\n",
    "            task_lbd_data_source = []   \n",
    "            for task_id, task_classes in enumerate(task_classes_arr):\n",
    "\n",
    "                train_mask = np.isin(label.cpu(), task_classes)\n",
    "                idx_train_task, x_train_task, y_train_task = index[train_mask], image[train_mask], label[train_mask]\n",
    "                task_lbd_data_source.append((idx_train_task, x_train_task, y_train_task))  #- (task_id * 2) #with overlap\n",
    "                  \n",
    "              \n",
    "\n",
    "             #create tensor dataset\n",
    "            for task_id, _ in enumerate(task_classes_arr):\n",
    "                self.task_id = task_id\n",
    "                train_lbd_data_source_task = TensorDataset(task_lbd_data_source[task_id][0], task_lbd_data_source[self.task_id][1], task_lbd_data_source[self.task_id][2])\n",
    "                train_lbd_loader_source_task = DataLoader(train_lbd_data_source_task, batch_size= batch_size_dict[\"labeled\"]) \n",
    "\n",
    "\n",
    "                self.set_attr(self.task_id, \"train_lbd_data_source_task\", train_lbd_data_source_task)  \n",
    "                self.set_attr(self.task_id, \"train_lbd_loader_source_task\", train_lbd_loader_source_task) \n",
    "\n",
    "                   \n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "        #........Start creating test unlabled target/task dataset   \n",
    "\n",
    "        test_suffix = \"test\" if self.config.data_params.train_val_split else \"\"\n",
    "\n",
    "        test_unl_dataset_target = create_dataset(\n",
    "            name,\n",
    "            domain[\"target\"],\n",
    "            suffix=test_suffix,\n",
    "            ret_index=True,\n",
    "            image_transform=raw,\n",
    "            image_size=image_size,\n",
    "        )\n",
    "\n",
    "\n",
    "        test_unl_loader_target = create_loader(\n",
    "            test_unl_dataset_target,\n",
    "            batch_size_dict[\"test\"],\n",
    "            is_train=False,\n",
    "            num_workers=num_workers,\n",
    "        )\n",
    "\n",
    "\n",
    "                                  \n",
    "             \n",
    "        #.......... Test unlabled target dataset task                   \n",
    "        x_test_target_idx = torch.tensor([0]).cuda()    \n",
    "        x_test_target_img = torch.zeros((1,3,224,224)).cuda()\n",
    "        x_test_target_lbl = torch.tensor([0]).cuda()\n",
    "\n",
    "        for idx , data in enumerate(test_unl_loader_target):\n",
    "\n",
    "            indx, img, label = data\n",
    "            indx = indx.cuda()\n",
    "            img = img.cuda()\n",
    "            label = label.cuda()\n",
    "\n",
    "            x_test_target_idx = torch.cat((x_test_target_idx, indx), 0)\n",
    "            x_test_target_img = torch.cat((x_test_target_img, img), 0)\n",
    "            x_test_target_lbl = torch.cat((x_test_target_lbl, label), 0)\n",
    "\n",
    "        image = x_test_target_img[1:]     \n",
    "        label = x_test_target_lbl[1:]\n",
    "        index = x_test_target_idx[1:]\n",
    "\n",
    "        # Divide the data over the different tasks\n",
    "        task_test_data_target = []   \n",
    "        for task_id, task_classes in enumerate(task_classes_arr):\n",
    "\n",
    "            test_mask = np.isin(label.cpu(), task_classes)\n",
    "            idx_test_task, x_test_task, y_test_task = index[test_mask], image[test_mask], label[test_mask]\n",
    "            task_test_data_target.append((idx_test_task, x_test_task, y_test_task))  \n",
    "\n",
    "\n",
    "\n",
    "        #create tensor dataset\n",
    "        for task_id, _ in enumerate(task_classes_arr):\n",
    "            self.task_id = task_id\n",
    "            test_unl_data_target_task = TensorDataset(task_test_data_target[self.task_id][0], task_test_data_target[self.task_id][1], task_test_data_target[self.task_id][2])\n",
    "            test_unl_loader_target_task = DataLoader(test_unl_data_target_task, batch_size= batch_size_dict[\"test\"]) \n",
    "\n",
    "            self.set_attr(self.task_id, \"test_unl_data_target_task\", test_unl_data_target_task)  \n",
    "            self.set_attr(self.task_id, \"test_unl_loader_target_task\", test_unl_loader_target_task) \n",
    "            test_unl_data_target_task = self.get_attr(task_id, \"test_unl_data_target_task\")\n",
    "            self.set_attr(task_id, \"test_len_data_target_task\", len(test_unl_data_target_task))\n",
    "           \n",
    "\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"Dataset {name}, source {self.config.data_params.source}, target {self.config.data_params.target}\"\n",
    "        )\n",
    "        \n",
    " \n",
    "    #................................Model\n",
    "    def _create_model(self):\n",
    "\n",
    "        version_grp = self.config.model_params.version.split(\"-\")\n",
    "     \n",
    "        version = version_grp[-1]\n",
    "        pretrained = \"pretrain\" in version_grp\n",
    "        if pretrained:\n",
    "            self.logger.info(\"Imagenet pretrained model used\")\n",
    "                                  \n",
    "        out_dim = self.config.model_params.out_dim  \n",
    "     \n",
    "        # backbone\n",
    "        if \"resnet\" in version:\n",
    "            net_class = getattr(torchvision.models, version)\n",
    "\n",
    "            if pretrained:\n",
    "                model = net_class(pretrained=pretrained)\n",
    "                model.fc = nn.Linear(model.fc.in_features, out_dim)\n",
    "                torchutils.weights_init(model.fc)\n",
    "            else:\n",
    "               \n",
    "                model = net_class(pretrained=False, num_class_task=out_dim)   \n",
    "                                  \n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "                                  \n",
    "        model = nn.DataParallel(model, device_ids=self.gpu_devices)  \n",
    "        model = model.cuda()\n",
    "        self.model = model \n",
    "                                  \n",
    "\n",
    "\n",
    "        if self.cls:\n",
    "            self.criterion = nn.CrossEntropyLoss().cuda()\n",
    "                \n",
    "            self.task_heads = []\n",
    "            for task_id in range(len(task_classes_arr)):\n",
    "                self.task_id = task_id\n",
    "                cls_head = CosineClassifier(\n",
    "        \n",
    "                    num_class = self.num_class_task, inc=out_dim, temp= self.config.loss_params.T  )  \n",
    "                self.task_heads.append(cls_head.cuda())\n",
    "                torchutils.weights_init(self.task_heads[self.task_id]) \n",
    "\n",
    "\n",
    "                \n",
    "    #.............................Optimizer                            \n",
    "    def _create_optimizer(self):\n",
    "        lr = self.config.optim_params.learning_rate\n",
    "        momentum = self.config.optim_params.momentum\n",
    "        weight_decay = self.config.optim_params.weight_decay\n",
    "        conv_lr_ratio = self.config.optim_params.conv_lr_ratio\n",
    "\n",
    "        parameters = []\n",
    "        # batch_norm layer: no weight_decay\n",
    "        params_bn, _ = torchutils.split_params_by_name(self.model, \"bn\") \n",
    "        parameters.append({\"params\": params_bn, \"weight_decay\": 0.0})\n",
    "        # conv layer: small lr\n",
    "        _, params_conv = torchutils.split_params_by_name(self.model, [\"fc\", \"bn\"]) \n",
    "        if conv_lr_ratio:\n",
    "            parameters[0][\"lr\"] = lr * conv_lr_ratio\n",
    "            parameters.append({\"params\": params_conv, \"lr\": lr * conv_lr_ratio})\n",
    "        else:\n",
    "            parameters.append({\"params\": params_conv})\n",
    "        # fc layer\n",
    "        params_fc, _ = torchutils.split_params_by_name(self.model, \"fc\") \n",
    "                \n",
    "        if self.cls and self.config.optim_params.cls_update:\n",
    "                                  \n",
    "        \n",
    "            params_fc.extend(list(self.task_heads[self.task_id].parameters()))\n",
    "           \n",
    "        parameters.append({\"params\": params_fc})\n",
    "\n",
    "                                  \n",
    "        self.optim = torch.optim.SGD(\n",
    "            parameters,\n",
    "            lr = lr,\n",
    "            weight_decay = weight_decay,\n",
    "            momentum=momentum,\n",
    "            nesterov=self.config.optim_params.nesterov,\n",
    "        )\n",
    "\n",
    "                \n",
    "        # lr schedular\n",
    "        if self.config.optim_params.lr_decay_schedule: \n",
    "            optim_stepLR = torch.optim.lr_scheduler.MultiStepLR(\n",
    "                self.optim,\n",
    "                milestones=self.config.optim_params.lr_decay_schedule,\n",
    "                gamma=self.config.optim_params.lr_decay_rate,\n",
    "            )\n",
    "            self.lr_scheduler_list.append(optim_stepLR)\n",
    "\n",
    "        if self.config.optim_params.decay:\n",
    "            self.optim_iterdecayLR = torchutils.lr_scheduler_invLR(self.optim)\n",
    "    \n",
    "         \n",
    "                \n",
    "    def train_one_epoch(self):\n",
    "    \n",
    "        loss_all_tasks = [[] for i in range(len(self.task_classes_arr))]                 \n",
    "        self.loss_all_tasks = loss_all_tasks\n",
    "        self.accs_naive =[]\n",
    "        clus_inf = []\n",
    "        self.clus_inf = clus_inf\n",
    "                                  \n",
    "   \n",
    "        self.len_task_src = np.zeros(self.task_num)\n",
    "        self.len_task_tgt = np.zeros(self.task_num)\n",
    "        \n",
    "        for task_id, task_classes in enumerate(task_classes_arr):\n",
    "            self.task_id = task_id\n",
    "            print(f\"Training starts for task {self.task_id}\" )\n",
    "            self.model = self.model.train()\n",
    "            #print(\"Model is in the train mode\")\n",
    "            loss_all_tasks[self.task_id].append({})\n",
    "\n",
    "\n",
    "            if self.cls:\n",
    "                self.task_heads[self.task_id].train()                   \n",
    "                                                        \n",
    "            self.loss_fn.module.epoch = self.current_epoch \n",
    "\n",
    "            loss_list = self.config.loss_params.loss\n",
    "            loss_weight = self.config.loss_params.weight\n",
    "            loss_warmup = self.config.loss_params.start\n",
    "            loss_giveup = self.config.loss_params.end\n",
    "\n",
    "            num_loss = len(loss_list) \n",
    "\n",
    "            task_source_loader = self.get_attr(self.task_id, \"train_loader_init_source_task\",)\n",
    "            task_target_loader = self.get_attr(self.task_id, \"train_loader_init_target_task\",) \n",
    "            \n",
    "            len_data_src_task = self.get_attr(self.task_id, \"train_len_init_source_task\")\n",
    "            len_data_tgt_task = self.get_attr(self.task_id, \"train_len_init_target_task\")\n",
    "            \n",
    "            self.len_task_src[self.task_id] =  len_data_src_task \n",
    "            self.len_task_tgt[self.task_id] =  len_data_tgt_task \n",
    "                \n",
    "            if self.config.steps_epoch is None:\n",
    "                num_batches = max(len(task_source_loader), len(task_target_loader)) + 1 \n",
    "                self.logger.info(f\"source task loader batches: {len(task_source_loader)}\")\n",
    "                self.logger.info(f\"target task loader batches: {len(task_target_loader)}\")\n",
    "            else:\n",
    "                num_batches = self.config.steps_epoch\n",
    "\n",
    "            epoch_loss = AverageMeter() \n",
    "            epoch_loss_parts = [AverageMeter() for _ in range(num_loss)]\n",
    "      \n",
    "            # cluster\n",
    "            if self.clus:\n",
    "                if self.config.loss_params.clus.kmeans_freq:    \n",
    "                    kmeans_batches = num_batches // self.config.loss_params.clus.kmeans_freq   \n",
    "                else:\n",
    "                    kmeans_batches = 1\n",
    "            else:\n",
    "                kmeans_batches = None\n",
    "\n",
    "                \n",
    "            # load weight\n",
    "            self._load_fewshot_to_cls_weight() \n",
    "            if self.fewshot:\n",
    "   \n",
    "                fewshot_index_tsk = torch.tensor(self.get_attr(self.task_id, \"fewshot_index_src_task\")).cuda()\n",
    "               \n",
    "                                         \n",
    "            tqdm_batch = tqdm( \n",
    "                total = num_batches, desc=f\"[Task {self.task_id} Epoch {self.current_epoch}]\", leave=False\n",
    "            )\n",
    "            tqdm_post = {} \n",
    "\n",
    "            for batch_i in range(num_batches):\n",
    "                # Kmeans\n",
    "                if is_div(kmeans_batches, batch_i):\n",
    "                    self._update_cluster_labels()\n",
    "                                  \n",
    "                if not self.config.optim_params.cls_update: \n",
    "                    self._load_fewshot_to_cls_weight() \n",
    "\n",
    "                # iteration over all source images\n",
    "                if not batch_i % len(task_source_loader):\n",
    "                    source_iter_tsk = iter(task_source_loader) \n",
    "\n",
    "                    if \"semi-condentmax\" in self.config.loss_params.loss: \n",
    "                        momentum_prob_source_task = (\n",
    "                            self.momentum_softmax_source_task[self.task_id].softmax_vector.cuda()\n",
    "                        )\n",
    "                        self.momentum_softmax_source_task[self.task_id].reset()\n",
    "\n",
    "                # iteration over all target images\n",
    "                if not batch_i % len(task_target_loader):\n",
    "                    target_iter_tsk = iter(task_target_loader) \n",
    "\n",
    "                    if \"tgt-condentmax\" in self.config.loss_params.loss:  \n",
    "                        momentum_prob_target_task = (\n",
    "                            self.momentum_softmax_target_task[self.task_id].softmax_vector.cuda()\n",
    "                        )\n",
    "                        self.momentum_softmax_target_task[self.task_id].reset()\n",
    "\n",
    "                \n",
    "                train_loader_lbd_task = self.get_attr(self.task_id, \"train_lbd_loader_source_task\")\n",
    "                # iteration over all labeled source images\n",
    "                if self.cls and not batch_i % len(train_loader_lbd_task): \n",
    "                    source_lbd_iter_task = iter(train_loader_lbd_task)\n",
    "\n",
    "                               \n",
    "              \n",
    "                # calculate loss\n",
    "                for domain_name in (\"source\", \"target\"):\n",
    "                    loss_tsk = torch.tensor(0).cuda()\n",
    "                    loss_d_tsk = 0\n",
    "                    loss_part_d_tsk = [0] * num_loss\n",
    "                    batch_size = self.batch_size_dict[domain_name]\n",
    "\n",
    "                    if self.cls and domain_name == \"source\":\n",
    "                        indices_lbld_tsk, images_lbd_tsk, labels_lbld_tsk = next(source_lbd_iter_task)\n",
    "                       \n",
    "                        if self.task_id == 0:\n",
    "                            indices_lbd_tsk = indices_lbld_tsk\n",
    "                            labels_lbd_tsk = labels_lbld_tsk\n",
    "                        else:\n",
    "                            indices_lbd_tsk = indices_lbld_tsk - torch.min(indices_lbld_tsk)\n",
    "                            labels_lbd_tsk = labels_lbld_tsk - torch.min(labels_lbld_tsk)\n",
    "                        \n",
    "                        indices_lbd_tsk = indices_lbd_tsk.cuda()\n",
    "                       \n",
    "                        images_lbd_tsk = images_lbd_tsk.cuda()\n",
    "                        labels_lbd_tsk = labels_lbd_tsk.cuda()\n",
    "                       \n",
    "                        feat_lbd_tsk = self.model(images_lbd_tsk)\n",
    "                        feat_lbd_tsk = F.normalize(feat_lbd_tsk, dim=1)\n",
    "                        out_lbd_tsk = self.task_heads[self.task_id](feat_lbd_tsk)\n",
    "\n",
    "\n",
    "                    # Matching & ssl\n",
    "                    if (self.tgt and domain_name == \"target\") or self.ssl:\n",
    "                        loader_iter_tsk = (\n",
    "                            source_iter_tsk if domain_name == \"source\" else target_iter_tsk\n",
    "                        )\n",
    "                       \n",
    "                        indices_unl_tsk, images_unl_tsk, _ = next(loader_iter_tsk)\n",
    "                        images_unl_tsk = images_unl_tsk.cuda()\n",
    "                       \n",
    "                        if domain_name == \"source\":\n",
    "                            if self.task_id == 0:\n",
    "                                index_unl_tsk = indices_unl_tsk \n",
    "                                \n",
    "                            else:\n",
    "                                \n",
    "                                index_unl_tsk= indices_unl_tsk - torch.tensor(np.sum(self.len_task_src[:self.task_id]))\n",
    "                              \n",
    "                        else :\n",
    "                            if self.task_id == 0:\n",
    "                                index_unl_tsk = indices_unl_tsk \n",
    "                            else:\n",
    "                             \n",
    "                                index_unl_tsk= indices_unl_tsk - torch.tensor(np.sum(self.len_task_tgt[:self.task_id]))\n",
    "                                \n",
    "                            \n",
    "                        #indices_unl_tsk = indices_unl_tsk.cuda()\n",
    "                        index_unl_tsk = index_unl_tsk.cuda()\n",
    "                        feat_unl_tsk = self.model(images_unl_tsk)\n",
    "                        feat_unl_tsk = F.normalize(feat_unl_tsk, dim=1)\n",
    "                        out_unl_tsk = self.task_heads[self.task_id](feat_unl_tsk)\n",
    "\n",
    "                    # Semi Supervised\n",
    "                    if self.semi and domain_name == \"source\":\n",
    "                       \n",
    "                        semi_mask_tsk = ~torchutils.isin(index_unl_tsk, fewshot_index_tsk)\n",
    "                    \n",
    "                        indices_semi_tsk = index_unl_tsk[semi_mask_tsk] \n",
    "                        \n",
    "                        out_semi_tsk = out_unl_tsk[semi_mask_tsk] \n",
    "\n",
    "                    # Self-supervised Learning\n",
    "                    if self.ssl:\n",
    "                        _, new_data_memory_tsk, loss_ssl_tsk, aux_list_tsk = self.loss_fn( \n",
    "                            self.task_id, index_unl_tsk.long(), feat_unl_tsk, domain_name, self.parallel_helper_idxs  \n",
    "                        )\n",
    "                        loss_ssl_tsk = [torch.mean(ls) for ls in loss_ssl_tsk] \n",
    "\n",
    "                    # pseudo\n",
    "                    loss_pseudo_tsk = torch.tensor(0).cuda()\n",
    "                    is_pseudo_tsk = {\"source\": self.is_pseudo_src, \"target\": self.is_pseudo_tgt}\n",
    "                    thres_dict = {\n",
    "                        \"source\": self.config.loss_params.thres_src, #Related to high confidence pseudo\n",
    "                        \"target\": self.config.loss_params.thres_tgt, #Related to high confidence pseudo\n",
    "                    }\n",
    "\n",
    "                    if is_pseudo_tsk[domain_name]:\n",
    "                        if domain_name == \"source\":\n",
    "                            indices_pseudo_tsk = indices_semi_tsk\n",
    "                            out_pseudo_tsk = out_semi_tsk\n",
    "                            pseudo_domain_tsk = self.get_attr(self.task_id, \"predict_ordered_labels_pseudo_source_tsk\")\n",
    "                            print(\"***pseudo_domain_tsk_source***\", pseudo_domain_tsk )\n",
    "                            print(\"*****Length of pseudo_domain_tsk_source**** \", len(pseudo_domain_tsk))\n",
    "                        else:\n",
    "                            indices_pseudo_tsk = index_unl_tsk\n",
    "                            out_pseudo_tsk = out_unl_tsk \n",
    "                            pseudo_domain_tsk = self.get_attr(self.task_id, \"predict_ordered_labels_pseudo_target_tsk\")\n",
    "                            print(\"***pseudo_domain_tsk_target***\", pseudo_domain_tsk )\n",
    "                            print(\"*****Length of pseudo_domain_tsk_target**** \", len(pseudo_domain_tsk))\n",
    "                        thres = thres_dict[domain_name] \n",
    "\n",
    "                        # calculate loss\n",
    "                        loss_pseudo_tsk, aux_tsk = torchutils.pseudo_label_loss( \n",
    "                            out_pseudo_tsk,\n",
    "                            thres=thres,\n",
    "                            mask=None,\n",
    "                            #num_class=self.num_class,\n",
    "                            num_class = self.num_class_task , \n",
    "                            aux=True\n",
    "                        )\n",
    "                        mask_pseudo_tsk = aux_tsk[\"mask\"] \n",
    "                       \n",
    "\n",
    "                        # fewshot memory bank\n",
    "                        mb_tsk = self.get_attr(self.task_id, \"memory_bank_wrapper_src\")\n",
    "                       \n",
    "                        indices_lbd_tounl_tsk = fewshot_index_tsk[indices_lbd_tsk] \n",
    "                        \n",
    "                        \n",
    "                        mb_feat_lbd_tsk = mb_tsk.at_idxs(indices_lbd_tounl_tsk.long()) \n",
    "        \n",
    "                        \n",
    "                        fewshot_data_memory_tsk = update_data_memory(mb_feat_lbd_tsk, feat_lbd_tsk) \n",
    "\n",
    "                        # stat\n",
    "                        pred_selected_tsk = out_pseudo_tsk.argmax(dim=1)[mask_pseudo_tsk] \n",
    "                          \n",
    "                        indices_selected_tsk = indices_pseudo_tsk[mask_pseudo_tsk] \n",
    "                        \n",
    "                        indices_unselected_tsk = indices_pseudo_tsk[~mask_pseudo_tsk] \n",
    "                        \n",
    "                        pseudo_domain_tsk[indices_selected_tsk.long()] = pred_selected_tsk \n",
    "                    \n",
    "                        pseudo_domain_tsk[indices_unselected_tsk.long()] = -1 \n",
    "                       \n",
    "                    # Compute Loss\n",
    "                    for ind, ls in enumerate(loss_list):  \n",
    "                        if (\n",
    "                            self.current_epoch < loss_warmup[ind] \n",
    "                            or self.current_epoch >= loss_giveup[ind] \n",
    "                        ):\n",
    "                            continue\n",
    "                        \n",
    "                        print(\"Compute loss is happening..\")\n",
    "                                  \n",
    "                        loss_part_tsk = torch.tensor(0).cuda()\n",
    "                        # *** handler for different loss ***\n",
    "                        # classification on few-shot\n",
    "                        if ls == \"cls-so\" and domain_name == \"source\":\n",
    "                            loss_part_tsk = self.criterion(out_lbd_tsk, labels_lbd_tsk)\n",
    "                                  \n",
    "                        elif ls == \"cls-info\" and domain_name == \"source\":\n",
    "                            loss_part_tsk = loss_info(feat_lbd_tsk, mb_feat_lbd_tsk, labels_lbd_tsk) \n",
    "                                  \n",
    "                        # semi-supervision learning on unlabled source\n",
    "                        elif ls == \"semi-entmin\" and domain_name == \"source\":\n",
    "                            loss_part_tsk = torchutils.entropy(out_semi_tsk) \n",
    "                                  \n",
    "                        elif ls == \"semi-condentmax\" and domain_name == \"source\":\n",
    "                            bs_tsk = out_semi_tsk.size(0)\n",
    "                            prob_semi_tsk = F.softmax(out_semi_tsk, dim=1)\n",
    "                            prob_mean_semi_tsk = prob_semi_tsk.sum(dim=0) / bs_tsk\n",
    "\n",
    "                            # update momentum\n",
    "                            self.momentum_softmax_source_task[self.task_id].update(  \n",
    "                                prob_mean_semi_tsk.cpu().detach(), bs_tsk\n",
    "                            )\n",
    "                            # get momentum probability\n",
    "                            momentum_prob_source_task = ( \n",
    "                                self.momentum_softmax_source_task[self.task_id].softmax_vector.cuda() \n",
    "                            )\n",
    "                                  \n",
    "                            # compute loss\n",
    "                            entropy_cond_tsk = -torch.sum( \n",
    "                                prob_mean_semi_tsk * torch.log(momentum_prob_source_task + 1e-5)\n",
    "                            )\n",
    "                            loss_part_tsk = -entropy_cond_tsk\n",
    "\n",
    "                        # learning on unlabeled target domain\n",
    "                        elif ls == \"tgt-entmin\" and domain_name == \"target\":\n",
    "                            loss_part_tsk = torchutils.entropy(out_unl_tsk)\n",
    "                                  \n",
    "                        elif ls == \"tgt-condentmax\" and domain_name == \"target\":\n",
    "                            bs_tsk = out_unl_tsk.size(0)\n",
    "                            prob_unl_tsk = F.softmax(out_unl_tsk, dim=1)\n",
    "                            prob_mean_unl_tsk = prob_unl_tsk.sum(dim=0) / bs_tsk\n",
    "\n",
    "                            # update momentum\n",
    "                            self.momentum_softmax_source_task[self.task_id].update(\n",
    "                                prob_mean_unl_tsk.cpu().detach(), bs_tsk\n",
    "                            )\n",
    "                            # get momentum probability\n",
    "                            momentum_prob_target_task = (\n",
    "                                self.momentum_softmax_target_task[self.task_id].softmax_vector.cuda()\n",
    "                            )\n",
    "                            # compute loss\n",
    "                            entropy_cond_tsk = -torch.sum(\n",
    "                                prob_mean_unl_tsk * torch.log(momentum_prob_target_task + 1e-5)\n",
    "                            )\n",
    "                            loss_part_tsk = -entropy_cond_tsk\n",
    "\n",
    "                        # self-supervised learning\n",
    "                        elif ls.split(\"-\")[0] in [\"ID\", \"CD\", \"proto\", \"I2C\", \"C2C\"]:\n",
    "                            loss_part_tsk = loss_ssl_tsk[ind] #!\n",
    "\n",
    "                        print(f\"***********End of loss computation for {domain_name}*******\")\n",
    "                        \n",
    "                        loss_part_tsk = loss_weight[ind] * loss_part_tsk\n",
    "                        loss_tsk = loss_tsk + loss_part_tsk\n",
    "                        loss_d_tsk = loss_d_tsk + loss_part_tsk.item()\n",
    "                        loss_part_d_tsk[ind] = loss_part_tsk.item()\n",
    "\n",
    "                                  \n",
    "                    # Backpropagation\n",
    "                    self.optim.zero_grad()\n",
    "                    if len(loss_list) and loss_tsk != 0:\n",
    "                        loss_tsk.backward()\n",
    "                    self.optim.step()\n",
    "\n",
    "                    # update memory_bank\n",
    "                    if self.ssl:\n",
    "                        self._update_memory_bank(domain_name, index_unl_tsk, new_data_memory_tsk) \n",
    "                        if domain_name == \"source\":\n",
    "                            self._update_memory_bank( \n",
    "                                domain_name, indices_lbd_tounl_tsk, fewshot_data_memory_tsk\n",
    "                            )\n",
    "\n",
    "                    # update lr info\n",
    "                    tqdm_post[\"lr\"] = torchutils.get_lr(self.optim, g_id=-1) \n",
    "\n",
    "                    # update loss info\n",
    "                    epoch_loss.update(loss_d_tsk, batch_size) \n",
    "                    tqdm_post[\"loss\"] = epoch_loss.avg \n",
    "                    self.summary_writer.add_scalars( \n",
    "                        \"train/loss\", {\"loss\": epoch_loss.val}, self.current_iteration\n",
    "                    )\n",
    "                    self.train_loss.append(epoch_loss.val)  \n",
    "                                  \n",
    "                    loss_all_tasks[task_id][0][domain_name] = [loss_part_tsk, loss_tsk, loss_d_tsk, loss_part_d_tsk, self.train_loss]\n",
    "\n",
    "\n",
    "                    # update loss part info\n",
    "                    domain_iteration = self.get_attr(domain_name, \"current_iteration\") \n",
    "                    self.summary_writer.add_scalars( \n",
    "                        f\"train/{self.domain_map[domain_name]}_loss\",\n",
    "                        {\"loss\": epoch_loss.val},\n",
    "                        domain_iteration,\n",
    "                    )\n",
    "                    for i, ls in enumerate(loss_part_d_tsk): \n",
    "                        ls_name = loss_list[i]\n",
    "                        epoch_loss_parts[i].update(ls, batch_size)\n",
    "                        tqdm_post[ls_abbr[ls_name]] = epoch_loss_parts[i].avg\n",
    "                        self.summary_writer.add_scalars(\n",
    "                            f\"train/{self.domain_map[domain_name]}_loss\",\n",
    "                            {ls_name: epoch_loss_parts[i].val},\n",
    "                            domain_iteration,\n",
    "                        )\n",
    "\n",
    "                    # adjust lr\n",
    "                    if self.config.optim_params.decay: \n",
    "                        self.optim_iterdecayLR.step()\n",
    "\n",
    "                    self.current_iteration += 1 \n",
    "                tqdm_batch.set_postfix(tqdm_post) \n",
    "                tqdm_batch.update() \n",
    "                self.current_iteration_source += 1\n",
    "                self.current_iteration_target += 1\n",
    "            tqdm_batch.close() \n",
    "\n",
    "            \n",
    "            self.current_loss = epoch_loss.avg\n",
    "                                  \n",
    "            loss_all_tasks[task_id].append(self.current_loss) \n",
    "    \n",
    "    \n",
    "            \n",
    "            val_acc_all_tasks = []\n",
    "            self.val_acc_all_tasks = val_acc_all_tasks\n",
    "            for idx in range(0, self.task_id + 1): \n",
    "                self.idx = idx\n",
    "                self.validate()\n",
    "                #self.save_checkpoint_tsk_val\n",
    "                \n",
    "            # For unseen tasks, we don't test\n",
    "            if self.task_id < (self.task_num - 1):\n",
    "                self.val_acc_all_tasks.extend([np.nan] * (7 - self.task_id))\n",
    "                # Collect all test accuracies\n",
    "            self.accs_naive.append(self.val_acc_all_tasks)\n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "                                  \n",
    "    @torch.no_grad()\n",
    "    def _load_fewshot_to_cls_weight(self):\n",
    "        \"\"\"load centroids to cosine classifier\n",
    "\n",
    "        Args:\n",
    "            method (str, optional): None, 'fewshot', 'src', 'tgt'. Defaults to None.\n",
    "        \"\"\"\n",
    "        method = self.config.model_params.load_weight\n",
    "        print(\"task_id in load fewshot:\" , self.task_id)\n",
    "        if method is None:\n",
    "            return\n",
    "        assert method in [\"fewshot\", \"src\", \"tgt\", \"src-tgt\", \"fewshot-tgt\"]\n",
    "\n",
    "        thres = {\"src\": 1, \"tgt\": self.config.model_params.load_weight_thres}\n",
    "#         bank = {\n",
    "#             \"src_0\": self.get_attr(0, \"memory_bank_wrapper_src\").as_tensor(),\n",
    "#             \"src_1\": self.get_attr(1, \"memory_bank_wrapper_src\").as_tensor(),\n",
    "#             \"src_2\": self.get_attr(2, \"memory_bank_wrapper_src\").as_tensor(),\n",
    "#             \"src_3\": self.get_attr(3, \"memory_bank_wrapper_src\").as_tensor(),\n",
    "#             \"src_4\": self.get_attr(4, \"memory_bank_wrapper_src\").as_tensor(),\n",
    "#             \"src_5\": self.get_attr(5, \"memory_bank_wrapper_src\").as_tensor(),\n",
    "#             \"tgt_0\": self.get_attr(0, \"memory_bank_wrapper_tgt\").as_tensor(),\n",
    "#             \"tgt_1\": self.get_attr(1, \"memory_bank_wrapper_tgt\").as_tensor(),\n",
    "#             \"tgt_2\": self.get_attr(2, \"memory_bank_wrapper_tgt\").as_tensor(),\n",
    "#             \"tgt_3\": self.get_attr(3, \"memory_bank_wrapper_tgt\").as_tensor(),\n",
    "#             \"tgt_4\": self.get_attr(4, \"memory_bank_wrapper_tgt\").as_tensor(),\n",
    "#             \"tgt_5\": self.get_attr(5, \"memory_bank_wrapper_tgt\").as_tensor(),\n",
    "#         }\n",
    "        \n",
    "        \n",
    "        bank = {\n",
    "        \"src\": {\n",
    "            0: self.get_attr(0, \"memory_bank_wrapper_src\").as_tensor(),\n",
    "            1: self.get_attr(1, \"memory_bank_wrapper_src\").as_tensor(),\n",
    "            2: self.get_attr(2, \"memory_bank_wrapper_src\").as_tensor(),\n",
    "            3: self.get_attr(3, \"memory_bank_wrapper_src\").as_tensor(),\n",
    "            4: self.get_attr(4, \"memory_bank_wrapper_src\").as_tensor(),\n",
    "            5: self.get_attr(5, \"memory_bank_wrapper_src\").as_tensor(),\n",
    "            6: self.get_attr(6, \"memory_bank_wrapper_src\").as_tensor(),\n",
    "            7: self.get_attr(7, \"memory_bank_wrapper_src\").as_tensor(),\n",
    "            },\n",
    "    \n",
    "        \"tgt\": {\n",
    "            0: self.get_attr(0, \"memory_bank_wrapper_tgt\").as_tensor(),\n",
    "            1: self.get_attr(1, \"memory_bank_wrapper_tgt\").as_tensor(),\n",
    "            2: self.get_attr(1, \"memory_bank_wrapper_tgt\").as_tensor(),\n",
    "            3: self.get_attr(3, \"memory_bank_wrapper_tgt\").as_tensor(),\n",
    "            4: self.get_attr(4, \"memory_bank_wrapper_tgt\").as_tensor(),\n",
    "            5: self.get_attr(5, \"memory_bank_wrapper_tgt\").as_tensor(),\n",
    "            6: self.get_attr(6, \"memory_bank_wrapper_tgt\").as_tensor(),\n",
    "            7: self.get_attr(7, \"memory_bank_wrapper_tgt\").as_tensor(),\n",
    "       }\n",
    "    }\n",
    "        \n",
    "        fewshot_label_tsk = {}\n",
    "        fewshot_index_tsk = {}\n",
    "        \n",
    "        is_tgt = (\n",
    "            method in [\"tgt\", \"fewshot-tgt\", \"src-tgt\"]\n",
    "            and self.current_epoch >= self.config.model_params.load_weight_epoch\n",
    "        )\n",
    "        \n",
    "        if method in [\"fewshot\", \"fewshot-tgt\"]:\n",
    "            if self.fewshot:\n",
    "\n",
    "                fewshot_index_tsk[\"src\"] : {\n",
    "                   \n",
    "                    0:torch.tensor(self.get_attr(0, \"fewshot_index_src_task\")),\n",
    "                    1:torch.tensor(self.get_attr(1, \"fewshot_index_src_task\")),\n",
    "                    2:torch.tensor(self.get_attr(2, \"fewshot_index_src_task\")),\n",
    "                    3:torch.tensor(self.get_attr(3, \"fewshot_index_src_task\")),\n",
    "                    4:torch.tensor(self.get_attr(4, \"fewshot_index_src_task\")),\n",
    "                    5:torch.tensor(self.get_attr(5, \"fewshot_index_src_task\")),\n",
    "                    6:torch.tensor(self.get_attr(6, \"fewshot_index_src_task\")),\n",
    "                    7:torch.tensor(self.get_attr(7, \"fewshot_index_src_task\")),\n",
    "                }\n",
    "                \n",
    "                fewshot_label_tsk[\"src\"] : {\n",
    "                   \n",
    "                    0:torch.tensor(self.get_attr(0, \"fewshot_label_src_task\")),\n",
    "                    1:torch.tensor(self.get_attr(1, \"fewshot_label_src_task\")),\n",
    "                    2:torch.tensor(self.get_attr(2, \"fewshot_label_src_task\")),\n",
    "                    3:torch.tensor(self.get_attr(3, \"fewshot_label_src_task\")),\n",
    "                    4:torch.tensor(self.get_attr(4, \"fewshot_label_src_task\")),\n",
    "                    5:torch.tensor(self.get_attr(5, \"fewshot_label_src_task\")),\n",
    "                    6:torch.tensor(self.get_attr(6, \"fewshot_label_src_task\")), \n",
    "                    7:torch.tensor(self.get_attr(7, \"fewshot_label_src_task\")),\n",
    "                }\n",
    "\n",
    "            else:\n",
    "\n",
    "                fewshot_label_tsk[\"src\"] : {\n",
    "            \n",
    "                    0:self.get_attr(0, \"train_ordered_labels_source_task\"),\n",
    "                    1:self.get_attr(1, \"train_ordered_labels_source_task\"),\n",
    "                    2:self.get_attr(2, \"train_ordered_labels_source_task\"),\n",
    "                    3:self.get_attr(3, \"train_ordered_labels_source_task\"),\n",
    "                    4:self.get_attr(4, \"train_ordered_labels_source_task\"),\n",
    "                    5:self.get_attr(5, \"train_ordered_labels_source_task\"),\n",
    "                    6:self.get_attr(6, \"train_ordered_labels_source_task\"),\n",
    "                    7:self.get_attr(7, \"train_ordered_labels_source_task\"),\n",
    "                     }\n",
    "                                          \n",
    "                fewshot_index_tsk[\"src\"] : {\n",
    "\n",
    "                    0:self.get_attr(0, \"train_len_source_task\"),\n",
    "                    1:self.get_attr(1, \"train_len_source_task\"),\n",
    "                    2:self.get_attr(2, \"train_len_source_task\"),\n",
    "                    3:self.get_attr(3, \"train_len_source_task\"),\n",
    "                    4:self.get_attr(4, \"train_len_source_task\"),\n",
    "                    5:self.get_attr(5, \"train_len_source_task\"),\n",
    "                    6:self.get_attr(6, \"train_len_source_task\"),\n",
    "                    7:self.get_attr(7, \"train_len_source_task\"),\n",
    "                     }\n",
    "\n",
    "\n",
    "        else:       \n",
    "\n",
    "\n",
    "\n",
    "            mask_tsk_0 = torch.tensor(self.get_attr(0, \"predict_ordered_labels_pseudo_source_tsk\"))!= -1 \n",
    "            mask_tsk_1 = torch.tensor(self.get_attr(1, \"predict_ordered_labels_pseudo_source_tsk\")) != -1 \n",
    "            mask_tsk_2 = torch.tensor(self.get_attr(2, \"predict_ordered_labels_pseudo_source_tsk\")) != -1 \n",
    "            mask_tsk_3 = torch.tensor(self.get_attr(3, \"predict_ordered_labels_pseudo_source_tsk\")) != -1 \n",
    "            mask_tsk_4 = torch.tensor(self.get_attr(4, \"predict_ordered_labels_pseudo_source_tsk\")) != -1 \n",
    "            mask_tsk_5 = torch.tensor(self.get_attr(5, \"predict_ordered_labels_pseudo_source_tsk\")) != -1 \n",
    "            mask_tsk_6 = torch.tensor(self.get_attr(6, \"predict_ordered_labels_pseudo_source_tsk\")) != -1 \n",
    "            mask_tsk_7 = torch.tensor(self.get_attr(7, \"predict_ordered_labels_pseudo_source_tsk\")) != -1 \n",
    "\n",
    "            fewshot_label_tsk[\"src\"] : {\n",
    "\n",
    "                0:self.get_attr(0, \"train_ordered_labels_source_task\")[mask_tsk_0],\n",
    "                1:self.get_attr(1, \"train_ordered_labels_source_task\")[mask_tsk_1],\n",
    "                2:self.get_attr(2, \"train_ordered_labels_source_task\")[mask_tsk_2],\n",
    "                3:self.get_attr(3, \"train_ordered_labels_source_task\")[mask_tsk_3],\n",
    "                4:self.get_attr(4, \"train_ordered_labels_source_task\")[mask_tsk_4],\n",
    "                5:self.get_attr(5, \"train_ordered_labels_source_task\")[mask_tsk_5],\n",
    "                6:self.get_attr(6, \"train_ordered_labels_source_task\")[mask_tsk_6],\n",
    "                7:self.get_attr(7, \"train_ordered_labels_source_task\")[mask_tsk_7],\n",
    "                 }\n",
    "            \n",
    "            \n",
    "            fewshot_index_tsk[\"src\"] : {\n",
    "\n",
    "                    0:mask_tsk_0.nonzero().squeeze(1),\n",
    "                    1:mask_tsk_1.nonzero().squeeze(1),\n",
    "                    2:mask_tsk_2.nonzero().squeeze(1),\n",
    "                    3:mask_tsk_3.nonzero().squeeze(1),\n",
    "                    4:mask_tsk_4.nonzero().squeeze(1),\n",
    "                    5:mask_tsk_5.nonzero().squeeze(1),\n",
    "                    6:mask_tsk_6.nonzero().squeeze(1),\n",
    "                    7:mask_tsk_7.nonzero().squeeze(1),\n",
    "                     }\n",
    "            \n",
    "            \n",
    "            \n",
    "        if is_tgt:\n",
    "\n",
    "\n",
    "\n",
    "            mask_tsk_0 = torch.tensor(self.get_attr(0, \"predict_ordered_labels_pseudo_target_tsk\")) != -1 \n",
    "            mask_tsk_1 = torch.tensor(self.get_attr(1, \"predict_ordered_labels_pseudo_target_tsk\")) != -1 \n",
    "            mask_tsk_2 = torch.tensor(self.get_attr(2, \"predict_ordered_labels_pseudo_target_tsk\")) != -1 \n",
    "            mask_tsk_3 = torch.tensor(self.get_attr(3, \"predict_ordered_labels_pseudo_target_tsk\")) != -1 \n",
    "            mask_tsk_4 = torch.tensor(self.get_attr(4, \"predict_ordered_labels_pseudo_target_tsk\")) != -1 \n",
    "            mask_tsk_5 = torch.tensor(self.get_attr(5, \"predict_ordered_labels_pseudo_target_tsk\")) != -1 \n",
    "            mask_tsk_6 = torch.tensor(self.get_attr(6, \"predict_ordered_labels_pseudo_target_tsk\")) != -1\n",
    "            mask_tsk_7 = torch.tensor(self.get_attr(7, \"predict_ordered_labels_pseudo_target_tsk\")) != -1\n",
    "            \n",
    "            fewshot_label_tsk[\"tgt\"]: {\n",
    "\n",
    "                0:self.get_attr(0, \"train_ordered_labels_target_task\")[mask_tsk_0], \n",
    "                1:self.get_attr(1, \"train_ordered_labels_target_task\")[mask_tsk_1],\n",
    "                2:self.get_attr(2, \"train_ordered_labels_target_task\")[mask_tsk_2],\n",
    "                3:self.get_attr(3, \"train_ordered_labels_target_task\")[mask_tsk_3],\n",
    "                4:self.get_attr(4, \"train_ordered_labels_target_task\")[mask_tsk_4],\n",
    "                5:self.get_attr(5, \"train_ordered_labels_target_task\")[mask_tsk_5],\n",
    "                6:self.get_attr(6, \"train_ordered_labels_target_task\")[mask_tsk_5],\n",
    "                7:self.get_attr(7, \"train_ordered_labels_target_task\")[mask_tsk_5],\n",
    "                 }\n",
    "            \n",
    "            \n",
    "            fewshot_index_tsk[\"tgt\"]: {\n",
    "\n",
    "                    0:mask_tsk_0.nonzero().squeeze(1),\n",
    "                    1:mask_tsk_1.nonzero().squeeze(1),\n",
    "                    2:mask_tsk_2.nonzero().squeeze(1),\n",
    "                    3:mask_tsk_3.nonzero().squeeze(1),\n",
    "                    4:mask_tsk_4.nonzero().squeeze(1),\n",
    "                    5:mask_tsk_5.nonzero().squeeze(1),\n",
    "                    6:mask_tsk_6.nonzero().squeeze(1),\n",
    "                    7:mask_tsk_7.nonzero().squeeze(1),\n",
    "\n",
    "                     }\n",
    "\n",
    "\n",
    "        \n",
    "        for task_id ,task_classes in enumerate(self.task_classes_arr):\n",
    "            task_id = self.task_id\n",
    "            weight = self.task_heads[self.task_id].fc.weight.data \n",
    "                \n",
    "            self.set_attr(self.task_id, \"weight_tsk\", weight)\n",
    "            \n",
    "            for domain in (\"src\", \"tgt\"):\n",
    "                if domain == \"tgt\" and not is_tgt:\n",
    "                    break\n",
    "                if domain == \"src\" and method == \"tgt\":\n",
    "                    break\n",
    "\n",
    "\n",
    "                    # for label in range(self.num_class):\n",
    "                    for label in (task_classes):  \n",
    "                        print(task_classes)\n",
    "                        fewshot_mask_tsk = fewshot_label_tsk[domain][self.task_id] == label\n",
    "                        if fewshot_mask_tsk.sum() < thres[domain]: \n",
    "                            continue\n",
    "                        fewshot_ind_tsk = fewshot_index_tsk[domain][self.task_id][fewshot_mask_tsk]\n",
    "                        bank_vec_tsk = bank[domain][self.task_id][fewshot_ind_tsk] \n",
    "                        weight_tsk = self.get_attr(self.task_id, \"wight_tsk\")\n",
    "                        weight[label] = F.normalize(torch.mean(bank_vec_tsk, dim=0), dim=0)\n",
    "\n",
    "    # Validate\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self):\n",
    "                                  \n",
    "\n",
    "        self.model.eval()\n",
    "        self.length_task_src = np.zeros(self.task_num)\n",
    "        \n",
    "        \n",
    "        print(f\"validation for task {self.idx} starts \")     \n",
    "        self.test_unl_loader_source_task = self.get_attr(self.idx, \"test_unl_loader_source_task\")\n",
    "        test_len_data_src_task = self.get_attr(self.idx, \"test_len_data_source_task\")\n",
    "        self.length_task_src[self.idx] = test_len_data_src_task \n",
    "        #print(\"length of tasks in source\",self.length_task_src[self.task_id] )\n",
    "            \n",
    "       \n",
    "            \n",
    "        self.test_unl_loader_target_task = self.get_attr(self.idx, \"test_unl_loader_target_task\")\n",
    "        # Domain Adaptation\n",
    "        if self.cls:\n",
    "            \n",
    "\n",
    "            self.task_heads[self.idx].eval()\n",
    "            if ( \n",
    "                self.config.data_params.fewshot\n",
    "                and self.config.data_params.name not in [\"visda17\", \"digits\"]\n",
    "            ):\n",
    "                print(\"***********Score calculation for source started\")\n",
    "                self.score( \n",
    "                       \n",
    "                        self.test_unl_loader_source_task ,\n",
    "                        name=f\"unlabeled {self.domain_map['source'] } task {self.idx}\",\n",
    "                    )\n",
    "            print(\"***********Score calculation for target started\")\n",
    "            self.current_val_metric = self.score( \n",
    "                self.test_unl_loader_target_task,\n",
    "                name=f\"unlabeled  {self.domain_map['target']}  task {self.idx}\",\n",
    "            )\n",
    "\n",
    "        # update information\n",
    "        self.current_val_iteration += 1 \n",
    "        if self.current_val_metric >= self.best_val_metric:\n",
    "            self.best_val_metric = self.current_val_metric\n",
    "            self.best_val_epoch = self.current_epoch\n",
    "            self.iter_with_no_improv = 0\n",
    "        else:\n",
    "            self.iter_with_no_improv += 1\n",
    "\n",
    "        self.val_acc.append(self.current_val_metric)                               \n",
    "        self.val_acc_all_tasks.append(self.current_val_metric)\n",
    "\n",
    "        self.clear_train_features() \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def score(self, loader, name=\"test\"): \n",
    "        correct = 0\n",
    "        size = 0\n",
    "        epoch_loss = AverageMeter()\n",
    "        error_indices = []  \n",
    "        confusion_matrix = torch.zeros(self.num_class_task, self.num_class_task, dtype=torch.long)  \n",
    "                \n",
    "        pred_score = []\n",
    "        pred_label = []\n",
    "        label = []\n",
    "\n",
    "        for batch_i, (indices, images, labels) in enumerate(loader):\n",
    "            images = images.cuda()\n",
    "   \n",
    "            labels = labels.cuda()\n",
    "        \n",
    "            if self.idx == 0:\n",
    "                indices_tsk = indices\n",
    "                labels_tsk = labels \n",
    "            else:\n",
    "             \n",
    "                indices_tsk = indices - torch.tensor(np.sum(self.length_task_src[:self.idx])) #.cpu().astype(int)\n",
    "                indices_tsk = indices_tsk.to(torch.int64)\n",
    "                labels_tsk = labels - torch.min(labels)\n",
    "                          \n",
    "            \n",
    "            \n",
    "            feat = self.model(images)\n",
    "            \n",
    "            feat = F.normalize(feat, dim=1)\n",
    "            output = self.task_heads[self.idx](feat)\n",
    "            \n",
    "            prob = F.softmax(output, dim=-1)\n",
    "            \n",
    "            loss = self.criterion(output,labels_tsk )\n",
    "\n",
    "            pred = torch.max(output, dim=1)[1]\n",
    "\n",
    "            pred_label.extend(pred.cpu().tolist())\n",
    "            label.extend(labels_tsk.cpu().tolist())\n",
    "            if self.num_class_task == 2:\n",
    "                pred_score.extend(prob[:, 1].cpu().tolist())\n",
    "\n",
    "            correct += pred.eq(labels_tsk).sum().item()\n",
    "            \n",
    "            \n",
    "            for t, p, ind in zip(labels_tsk, pred, indices_tsk):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "                if t != p:\n",
    "                    error_indices.append((ind, p))\n",
    "            size += pred.size(0)\n",
    "            epoch_loss.update(loss, pred.size(0))\n",
    "\n",
    "        acc = correct / size\n",
    "        self.summary_writer.add_scalars(\n",
    "            \"test/acc\", {f\"{name}\": acc}, self.current_epoch\n",
    "        )\n",
    "        self.summary_writer.add_scalars(\n",
    "            \"test/loss\", {f\"{name}\": epoch_loss.avg}, self.current_epoch\n",
    "        )\n",
    "        self.logger.info(\n",
    "            f\"[Epoch {self.current_epoch} task {self.idx} {name}] loss={epoch_loss.avg:.5f}, acc={correct}/{size}({100. * acc:.3f}%)\"\n",
    "        )\n",
    "\n",
    "        return acc\n",
    "\n",
    "    # Load & Save checkpoint\n",
    "\n",
    "    def load_checkpoint(\n",
    "        self,\n",
    "        filename,\n",
    "        checkpoint_dir=None,\n",
    "        load_memory_bank=False,\n",
    "        load_model=True,\n",
    "        load_optim=False,\n",
    "        load_epoch=False,\n",
    "        load_cls=True,\n",
    "    ):\n",
    "        checkpoint_dir = checkpoint_dir or self.config.checkpoint_dir\n",
    "        filename = os.path.join(checkpoint_dir, filename)\n",
    "        try:\n",
    "            self.logger.info(f\"Loading checkpoint '{filename}'\")\n",
    "            checkpoint = torch.load(filename, map_location=\"cpu\")\n",
    "\n",
    "            if load_epoch:\n",
    "                self.current_epoch = checkpoint[\"epoch\"]\n",
    "                for domain_name in (\"source\", \"target\"):\n",
    "                    self.set_attr(\n",
    "                        domain_name,\n",
    "                        \"current_iteration\",\n",
    "                        checkpoint[f\"iteration_{domain_name}\"],\n",
    "                    )\n",
    "                self.current_iteration = checkpoint[\"iteration\"]\n",
    "                self.current_val_iteration = checkpoint[\"val_iteration\"]\n",
    "\n",
    "            if load_model:\n",
    "                model_state_dict = checkpoint[\"model_state_dict\"]\n",
    "                self.model.load_state_dict(model_state_dict)\n",
    "\n",
    "            if load_cls and self.cls and \"cls_state_dict\" in checkpoint:\n",
    "                cls_state_dict = checkpoint[\"cls_state_dict\"]\n",
    "                #self.cls_head.load_state_dict(cls_state_dict)\n",
    "                self.task_heads[self.task_id].load_state_dict(cls_state_dict)\n",
    "\n",
    "            if load_optim:\n",
    "                optim_state_dict = checkpoint[\"optim_state_dict\"]\n",
    "                self.optim.load_state_dict(optim_state_dict)\n",
    "\n",
    "                lr_pretrained = self.optim.param_groups[0][\"lr\"]\n",
    "                lr_config = self.config.optim_params.learning_rate\n",
    "\n",
    "                # Change learning rate\n",
    "                if not lr_pretrained == lr_config:\n",
    "                    for param_group in self.optim.param_groups:\n",
    "                        param_group[\"lr\"] = self.config.optim_params.learning_rate\n",
    "\n",
    "            self._init_memory_bank()\n",
    "            if (\n",
    "                load_memory_bank or self.config.model_params.load_memory_bank == False\n",
    "            ):  \n",
    "                self._load_memory_bank(\n",
    "                    {\n",
    "                        \"source\": checkpoint[\"memory_bank_source\"],\n",
    "                        \"target\": checkpoint[\"memory_bank_target\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"Checkpoint loaded successfully from '{filename}' at (epoch {checkpoint['epoch']}) at (iteration s:{checkpoint['iteration_source']} t:{checkpoint['iteration_target']}) with loss = {checkpoint['loss']}\\nval acc = {checkpoint['val_acc']}\\n\"\n",
    "            )\n",
    "\n",
    "        except OSError as e:\n",
    "            self.logger.info(f\"Checkpoint doesnt exists: [{filename}]\")\n",
    "            raise e\n",
    "            \n",
    "            \n",
    "    def save_checkpoint_tsk_train(self, filename=\"checkpoint_task_train.pth.tar\"):  \n",
    "        \n",
    "        out_dict_tsk = {\n",
    "            \"model_state_dict\": self.model.state_dict(), \n",
    "            \"optim_state_dict\": self.optim.state_dict(),\n",
    "            \"memory_bank_source_tsk_0\": self.get_attr(0, \"memory_bank_wrapper_src\"),\n",
    "            \"memory_bank_source_tsk_1\": self.get_attr(1, \"memory_bank_wrapper_src\"),\n",
    "            \"memory_bank_source_tsk_2\": self.get_attr(2, \"memory_bank_wrapper_src\"),\n",
    "            \"memory_bank_source_tsk_3\": self.get_attr(3, \"memory_bank_wrapper_src\"),\n",
    "            \"memory_bank_source_tsk_4\": self.get_attr(4, \"memory_bank_wrapper_src\"),\n",
    "            \"memory_bank_source_tsk_5\": self.get_attr(5, \"memory_bank_wrapper_src\"),\n",
    "            \"memory_bank_target_tsk_0\": self.get_attr(0, \"memory_bank_wrapper_tgt\"),  \n",
    "            \"memory_bank_target_tsk_1\": self.get_attr(1, \"memory_bank_wrapper_tgt\"),\n",
    "            \"memory_bank_target_tsk_2\": self.get_attr(2, \"memory_bank_wrapper_tgt\"),\n",
    "            \"memory_bank_target_tsk_3\": self.get_attr(3, \"memory_bank_wrapper_tgt\"),\n",
    "            \"memory_bank_target_tsk_4\": self.get_attr(4, \"memory_bank_wrapper_tgt\"),\n",
    "            \"memory_bank_target_tsk_5\": self.get_attr(5, \"memory_bank_wrapper_tgt\"),\n",
    "            \"memory_bank_target_tsk_6\": self.get_attr(6, \"memory_bank_wrapper_tgt\"),\n",
    "            \"memory_bank_target_tsk_7\": self.get_attr(7, \"memory_bank_wrapper_tgt\"),\n",
    "            \n",
    "            \"iteration_source\": self.get_attr(\"source\", \"current_iteration\"), \n",
    "            \"iteration_target\": self.get_attr(\"target\", \"current_iteration\"), \n",
    "            \"loss\": self.current_loss,\n",
    "            \"train_loss\": np.array(self.train_loss),\n",
    "            \n",
    "        }\n",
    "        if self.cls:\n",
    "            out_dict[\"cls_state_dict\"] = cls_dict = {\n",
    "                \n",
    "                \"head_task_0\" : self.task_heads[0].state_dict(),\n",
    "                \"head_task_1\" : self.task_heads[1].state_dict(),\n",
    "                \"head_task_2\" : self.task_heads[2].state_dict(),  \n",
    "                \"head_task_3\" : self.task_heads[3].state_dict(),\n",
    "                \"head_task_4\" : self.task_heads[4].state_dict(),\n",
    "                \"head_task_5\" : self.task_heads[5].state_dict(),\n",
    "                \"head_task_6\" : self.task_heads[6].state_dict(),\n",
    "                \"head_task_7\" : self.task_heads[7].state_dict(),\n",
    "             }\n",
    "       \n",
    "        \n",
    "        torchutils.save_checkpoint(\n",
    "            out_dict, filename=filename, folder=self.config.checkpoint_dir\n",
    "        )\n",
    "        self.copy_checkpoint_tsk_train()\n",
    "\n",
    "        \n",
    "        \n",
    "    def save_checkpoint_tsk_val(self, filename=\"checkpoint_task_val.pth.tar\"):  \n",
    "        \n",
    "        out_dict_tsk = {\n",
    "            \"model_state_dict\": self.model.state_dict(), \n",
    "            \"optim_state_dict\": self.optim.state_dict(),\n",
    "            \"val_iteration\": self.current_val_iteration,\n",
    "            \"val_acc\": np.array(self.val_acc),\n",
    "            \"val_metric\": self.current_val_metric,\n",
    "            \n",
    "        }\n",
    " \n",
    "        if self.cls:\n",
    "            out_dict[\"cls_state_dict\"] = cls_dict = {\n",
    "                \n",
    "                \"head_task_0\" : self.task_heads[0].state_dict(),\n",
    "                \"head_task_1\" : self.task_heads[1].state_dict(),\n",
    "                \"head_task_2\" : self.task_heads[2].state_dict(),  \n",
    "                \"head_task_3\" : self.task_heads[3].state_dict(),\n",
    "                \"head_task_4\" : self.task_heads[4].state_dict(),\n",
    "                \"head_task_5\" : self.task_heads[5].state_dict(),\n",
    "                \"head_task_6\" : self.task_heads[5].state_dict(),\n",
    "                \"head_task_7\" : self.task_heads[5].state_dict(),\n",
    "                                                    \n",
    "                 }\n",
    "       \n",
    "        \n",
    "       \n",
    "        is_best = (\n",
    "            self.current_val_metric == self.best_val_metric\n",
    "        ) or not self.config.validate_freq\n",
    "        torchutils.save_checkpoint(\n",
    "            out_dict, is_best, filename=filename, folder=self.config.checkpoint_dir\n",
    "        )\n",
    "        self.copy_checkpoint_tsk_val()\n",
    "\n",
    "\n",
    "    def save_checkpoint(self, filename=\"checkpoint.pth.tar\"):\n",
    "        out_dict = {\n",
    "            \"config\": self.config,\n",
    "            \"model_state_dict\": self.model.state_dict(), \n",
    "            \"optim_state_dict\": self.optim.state_dict(),\n",
    "            \"epoch\": self.current_epoch,\n",
    "            \"iteration\": self.current_iteration,\n",
    "            \"iteration_source\": self.get_attr(\"source\", \"current_iteration\"), #fix it\n",
    "            \"iteration_target\": self.get_attr(\"target\", \"current_iteration\"), #fix it\n",
    "            \"val_iteration\": self.current_val_iteration,\n",
    "            \"val_acc\": np.array(self.val_acc),\n",
    "            \"val_metric\": self.current_val_metric,\n",
    "            \"loss\": self.current_loss,\n",
    "            \"loss_all_tasks\":self.loss_all_tasks,\n",
    "            \"clus_inf\" : self.clus_inf,\n",
    "            \"val_acc_all_tasks\": self.val_acc_all_tasks,\n",
    "            \"train_loss\": np.array(self.train_loss),\n",
    "            \n",
    "        }\n",
    "       \n",
    "          \n",
    "        # best according to source-to-target\n",
    "        is_best = (\n",
    "            self.current_val_metric == self.best_val_metric\n",
    "        ) or not self.config.validate_freq\n",
    "        torchutils.save_checkpoint(\n",
    "            out_dict, is_best, filename=filename, folder=self.config.checkpoint_dir\n",
    "        )\n",
    "        self.copy_checkpoint()\n",
    "\n",
    "    # compute train features\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_train_features(self):\n",
    "        #print(\"feature computation is running\")\n",
    "        if self.is_features_computed and self.task_idm not in [0,1,2,3,4,5,6,7] :\n",
    "            #print(\"feature computed return:\" ,self.is_features_computed)\n",
    "            return\n",
    "        else:\n",
    "            self.is_features_computed = True  \n",
    "            #print(\"feature computed model.eval:\" ,self.is_features_computed)\n",
    "        self.model.eval()\n",
    "        \n",
    "            \n",
    "        for domain in (\"source\", \"target\"):\n",
    "            #print(\"task_id:\", self.task_idm)\n",
    "            if domain == \"source\":   \n",
    "                #print(\"task_id\",self.task_id)\n",
    "                train_loader_source_tsk = self.get_attr(self.task_idm, \"train_loader_init_source_task\")\n",
    "                \n",
    "                self.length_task = np.zeros(self.task_num)\n",
    "                self.length_task[self.task_idm]  = self.get_attr(self.task_idm, \"train_len_init_source_task\")\n",
    "\n",
    "                features, y, idx = [], [], []\n",
    "                tqdm_batch = tqdm(\n",
    "                    total=len( train_loader_source_tsk), desc=f\"[Compute train features of task {self.task_idm} for {domain}]\"\n",
    "                )\n",
    "\n",
    "                for batch_i, (indices, images, labels) in enumerate(train_loader_source_tsk):\n",
    "         \n",
    "                    indices = indices.to(torch.int64)\n",
    "                    images = images.to(self.device)\n",
    "                    feat = self.model(images)\n",
    "                    feat = F.normalize(feat, dim=1)\n",
    "\n",
    "                    features.append(feat)\n",
    "                    y.append(labels)\n",
    "                    idx.append(indices)\n",
    "        \n",
    "\n",
    "                    tqdm_batch.update()\n",
    "                tqdm_batch.close()\n",
    "\n",
    "\n",
    "                features = torch.cat(features)\n",
    "\n",
    "                y = torch.cat(y)\n",
    "                idx = torch.cat(idx) \n",
    "                \n",
    "                \n",
    "                #.iterate over all three lists at the same time and sort the indices\n",
    "                data_zip = list(zip(idx, features, y))            \n",
    "                data_sort = sorted(data_zip)\n",
    "                \n",
    "\n",
    "                #....Devide the tupel into three lists\n",
    "                idx_sort = []\n",
    "                y_sort =[]\n",
    "                features_sort = []\n",
    "\n",
    "                for item in data_sort :\n",
    "                    idx_sort.append(item[0])\n",
    "                    features_sort.append(item[1])\n",
    "                    y_sort.append(item[2])\n",
    "\n",
    "                #...Convert list to torch.Tensor\n",
    "                idx_sort = torch.Tensor(idx_sort).to(self.device)  \n",
    "                y_sort = torch.Tensor(y_sort)\n",
    "                features_sort_stack = torch.stack(features_sort)\n",
    "\n",
    "\n",
    "                self.set_attr(self.task_idm, \"train_features_src_tsk\", features_sort_stack)\n",
    "                self.set_attr(self.task_idm, \"train_labels_scr_task\", y_sort) \n",
    "                self.set_attr(self.task_idm, \"train_indices_src_tsk\", idx_sort) \n",
    "\n",
    "            #....................for target domain\n",
    "            if domain == \"target\":   \n",
    "                train_loader_target_tsk = self.get_attr(self.task_idm, \"train_loader_init_target_task\")\n",
    "                features, y, idx = [], [], []\n",
    "                tqdm_batch = tqdm(\n",
    "                    total=len(train_loader_target_tsk), desc=f\"[Compute train features of task {self.task_idm} for {domain}]\"\n",
    "                )\n",
    "                for batch_i, (indices, images, labels) in enumerate(train_loader_target_tsk):\n",
    "\n",
    "                    indices = indices.to(torch.int64)\n",
    "                    images = images.to(self.device)\n",
    "                    feat = self.model(images)            \n",
    "                    feat = F.normalize(feat, dim=1)\n",
    "\n",
    "\n",
    "                    features.append(feat) \n",
    "                    y.append(labels)                 \n",
    "                    idx.append(indices)\n",
    "\n",
    "                    tqdm_batch.update()\n",
    "                tqdm_batch.close()\n",
    "\n",
    "                features = torch.cat(features)\n",
    "                y = torch.cat(y)       \n",
    "                idx = torch.cat(idx) \n",
    "              \n",
    "\n",
    "\n",
    "\n",
    "                #.iterate over all three lists at the same time and sort the indices\n",
    "                data_zip = list(zip(idx, features, y))              \n",
    "                data_sort = sorted(data_zip)\n",
    "\n",
    "                #....Devide the tupel into three lists\n",
    "                idx_sort = []\n",
    "                y_sort =[]\n",
    "                features_sort = []\n",
    "\n",
    "                for item in data_sort :\n",
    "                    idx_sort.append(item[0])\n",
    "                    features_sort.append(item[1])\n",
    "                    y_sort.append(item[2])\n",
    "\n",
    "   \n",
    "\n",
    "                #...Convert list to torch.Tensor\n",
    "                idx_sort = torch.Tensor(idx_sort).to(self.device)\n",
    "                y_sort = torch.Tensor(y_sort)\n",
    "                #features_sort = torch.Tensor(features_sort) \n",
    "                features_sort_stack = torch.stack(features_sort)\n",
    "                \n",
    "\n",
    "\n",
    "                self.set_attr(self.task_idm, \"train_features_tgt_tsk\", features_sort_stack) \n",
    "                self.set_attr(self.task_idm, \"train_labels_tgt_task\", y_sort) \n",
    "                self.set_attr(self.task_idm, \"train_indices_tgt_tsk\", idx_sort) \n",
    "                \n",
    "\n",
    "\n",
    "    def clear_train_features(self):\n",
    "        self.is_features_computed = False\n",
    "\n",
    "   \n",
    "               \n",
    "    #.define a single memeory for each task in each domain --> 12 memories            \n",
    "    @torch.no_grad()\n",
    "    def _init_memory_bank(self): #\n",
    "        out_dim = self.config.model_params.out_dim\n",
    "        self.length_task_source = np.zeros(self.task_num)\n",
    "        self.length_task_target = np.zeros(self.task_num)\n",
    "        for task_idm, _ in enumerate(self.task_classes_arr):\n",
    "            self.task_idm = task_idm\n",
    "            print(\"Task_idm for memory\", self.task_idm)\n",
    "            \n",
    "            for domain_name in (\"source\", \"target\"):\n",
    "\n",
    "                if domain_name == \"source\":\n",
    "\n",
    "                    data_len_source_task = self.get_attr(self.task_idm, \"train_len_init_source_task\")\n",
    "                    print(f\"data_len_source_task_{task_idm}\", data_len_source_task )              \n",
    "                    self.length_task_source[self.task_idm] = data_len_source_task\n",
    "                    \n",
    "                    \n",
    "                    memory_bank_source_task = MemoryBank(data_len_source_task, out_dim) #data_len: length of the memory bank, out_dim: dimension of the memory bank features\n",
    "                    \n",
    "                    if self.config.model_params.load_memory_bank:\n",
    "                        self.compute_train_features()  \n",
    "                        idx = self.get_attr(self.task_idm, \"train_indices_src_tsk\") \n",
    "                        feat = self.get_attr(self.task_idm, \"train_features_src_tsk\") \n",
    "                        \n",
    "                        if self.task_idm == 0:\n",
    "                            idx_n = idx \n",
    "                        else:\n",
    "                            #print(\"tensor print \", torch.tensor(np.sum(self.length_task_source[:self.task_idm])))\n",
    "                            idx_n = idx - torch.tensor(np.sum(self.length_task_source[:self.task_idm])) #.cpu().astype(int)\n",
    "             \n",
    "                        memory_bank_source_task.update(idx_n.to(torch.int64), feat)\n",
    "\n",
    "              \n",
    "                        if self.config.data_params.name in [\"visda17\", \"domainnet\"]:\n",
    "                            delattr(self, f\"train_indices_{domain_name}\")\n",
    "                            delattr(self, f\"train_features_{domain_name}\")\n",
    "                            \n",
    "\n",
    "                    \n",
    "                    self.set_attr(self.task_idm, \"memory_bank_wrapper_src\", memory_bank_source_task)        \n",
    "                    self.loss_fn.module.set_attr( self.task_idm, \"data_len_source_task\", data_len_source_task)                                 \n",
    "                    self.loss_fn.module.set_broadcast(\n",
    "                       task_idm, \"memory_bank_source_task\", memory_bank_source_task.as_tensor()\n",
    "                    )\n",
    "\n",
    "\n",
    "                if domain_name == \"target\":\n",
    "\n",
    "    \n",
    "                    data_len_target_task = self.get_attr(self.task_idm, \"train_len_init_target_task\")\n",
    "                    #print(\"data_len_target_task\", data_len_target_task)\n",
    "            \n",
    "                    self.length_task_target[self.task_idm] = data_len_target_task\n",
    "            \n",
    "                    memory_bank_target_task = MemoryBank(data_len_target_task, out_dim) \n",
    "\n",
    "                    if self.config.model_params.load_memory_bank:\n",
    "                        self.compute_train_features()\n",
    "                        idx = self.get_attr(self.task_idm, \"train_indices_tgt_tsk\") \n",
    "                        feat = self.get_attr(self.task_idm, \"train_features_tgt_tsk\") \n",
    "                        \n",
    "    \n",
    "                        if self.task_idm == 0:\n",
    "                            idx_n = idx \n",
    "                        else:\n",
    "                            #print(\"tensor print target\", torch.tensor(np.sum(self.length_task_target[:self.task_idm])))\n",
    "                            idx_n = idx - torch.tensor(np.sum(self.length_task_target[:self.task_idm])) #.cpu().astype(int)\n",
    "                          \n",
    "                        memory_bank_target_task.update(idx_n.to(torch.int64), feat) #memory_bank_target_task.update(idx.to(torch.int64), feat)\n",
    "                     \n",
    "        \n",
    "        \n",
    "                        if self.config.data_params.name in [\"visda17\", \"domainnet\"]:\n",
    "                            delattr(self, f\"train_indices_{domain_name}\")\n",
    "                            delattr(self, f\"train_features_{domain_name}\")\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    self.set_attr(self.task_idm, \"memory_bank_wrapper_tgt\", memory_bank_target_task)\n",
    "                    self.loss_fn.module.set_attr(self.task_idm, \"data_len_target_task\", data_len_target_task)              \n",
    "                    self.loss_fn.module.set_broadcast( \n",
    "                        self.task_idm, \"memory_bank_target_task\", memory_bank_target_task.as_tensor()\n",
    "                    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "    @torch.no_grad()\n",
    "    def _update_memory_bank(self, domain_name, indices, new_data_memory_tsk): #Fix it\n",
    "        \n",
    "        if domain_name == \"source\":\n",
    "            memory_bank_wrapper_src = self.get_attr(self.task_id, \"memory_bank_wrapper_src\")\n",
    "            memory_bank_wrapper_src.update(indices, new_data_memory_tsk)\n",
    "            updated_bank_src_tsk = memory_bank_wrapper_src.as_tensor()\n",
    "            self.loss_fn.module.set_broadcast(self.task_id, \"memory_bank_source_task\", updated_bank_src_tsk)  #Not sure\n",
    "\n",
    "        \n",
    "        if domain_name == \"taregt\":\n",
    "            memory_bank_wrapper_tgt = self.get_attr(self.task_id, \"memory_bank_wrapper_tgt\")\n",
    "            memory_bank_wrapper_tgt.update(indices, new_data_memory_tsk)\n",
    "            updated_bank_tgt_tsk = memory_bank_wrapper_tgt.as_tensor()\n",
    "            self.loss_fn.module.set_broadcast(self.task_id, \"memory_bank_target_task\", updated_bank_tgt_tsk)  #Not sure\n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "    def _load_memory_bank(self, memory_bank_dict):\n",
    "        \"\"\"load memory bank from checkpoint\n",
    "\n",
    "        Args:\n",
    "            memory_bank_dict (dict): memory_bank dict of source and target domain\n",
    "        \"\"\"\n",
    "        for domain_name in (\"source\", \"target\"):\n",
    "                                  \n",
    "            if domain_name == \"source\":       \n",
    "                memory_bank_src = memory_bank_dict[domain_name]._bank.cuda()\n",
    "                self.get_attr(self.task_id, \"memory_bank_wrapper_src\")._bank = memory_bank\n",
    "                self.loss_fn.module.set_broadcast(task_id, \"memory_bank\", memory_bank)\n",
    "                                  \n",
    "            if domain_name == \"target\":    \n",
    "                memory_bank_tgt = memory_bank_dict[domain_name]._bank.cuda() \n",
    "                self.get_attr(self.task_id, \"memory_bank_wrapper_tgt\")._bank = memory_bank_tgt\n",
    "                self.loss_fn.module.set_broadcast(self.task_id, \"memory_bank\", memory_bank)\n",
    "\n",
    "    # Cluster\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _update_cluster_labels(self):   #FIX it\n",
    "        #k_list = self.config.k_list \n",
    "        k_list_task = self.config.k_list_task\n",
    "        \n",
    "        for clus_type in self.config.loss_params.clus.type:\n",
    "            cluster_labels_domain = {}\n",
    "            cluster_centroids_domain = {}\n",
    "            cluster_phi_domain = {}\n",
    "\n",
    "            # clustering for each domain\n",
    "            if clus_type == \"each\":\n",
    "                for domain_name in (\"source\", \"target\"):\n",
    "                    \n",
    "                    if domain_name == \"source\":\n",
    "\n",
    "                        memory_bank_source_tensor = self.get_attr(\n",
    "                            self.task_id, \"memory_bank_wrapper_src\"\n",
    "                        ).as_tensor()\n",
    "\n",
    "                        # clustering\n",
    "                        cluster_labels, cluster_centroids, cluster_phi = torch_kmeans(\n",
    "                            k_list_task,\n",
    "                            memory_bank_source_tensor,\n",
    "                            seed=self.current_epoch + self.current_iteration,\n",
    "                        )\n",
    "\n",
    "                        cluster_labels_domain[domain_name] = cluster_labels\n",
    "                        cluster_centroids_domain[domain_name] = cluster_centroids\n",
    "                        cluster_phi_domain[domain_name] = cluster_phi\n",
    "\n",
    "                    if domain_name == \"target\":\n",
    "\n",
    "                        memory_bank_target_tensor = self.get_attr(\n",
    "                            self.task_id, \"memory_bank_wrapper_tgt\"\n",
    "                        ).as_tensor()\n",
    "\n",
    "                        # clustering\n",
    "                        cluster_labels, cluster_centroids, cluster_phi = torch_kmeans(\n",
    "                            k_list_task,\n",
    "                            memory_bank_target_tensor,\n",
    "                            seed=self.current_epoch + self.current_iteration,\n",
    "                        )\n",
    "\n",
    "                        cluster_labels_domain[domain_name] = cluster_labels\n",
    "                        cluster_centroids_domain[domain_name] = cluster_centroids\n",
    "                        cluster_phi_domain[domain_name] = cluster_phi  \n",
    "\n",
    "\n",
    "                        \n",
    "                        \n",
    "                self.cluster_each_centroids_domain = cluster_centroids_domain\n",
    "                self.cluster_each_labels_domain = cluster_labels_domain\n",
    "                self.cluster_each_phi_domain = cluster_phi_domain\n",
    "\n",
    "                #update task_clus_info\n",
    "                self.clus_inf.append([cluster_centroids_domain, cluster_labels_domain, cluster_phi_domain])\n",
    "            else:\n",
    "                print(clus_type)\n",
    "                raise NotImplementedError\n",
    "\n",
    "            # update cluster to losss_fn\n",
    "            for domain_name in (\"source\", \"target\"):  \n",
    "                self.loss_fn.module.set_broadcast(\n",
    "                    domain_name,\n",
    "                    f\"cluster_labels_{clus_type}\",\n",
    "                    cluster_labels_domain[domain_name],\n",
    "                )\n",
    "                self.loss_fn.module.set_broadcast(\n",
    "                    domain_name,\n",
    "                    f\"cluster_centroids_{clus_type}\",\n",
    "                    cluster_centroids_domain[domain_name],\n",
    "                )\n",
    "                if cluster_phi_domain:\n",
    "                    self.loss_fn.module.set_broadcast(\n",
    "                        domain_name,\n",
    "                        f\"cluster_phi_{clus_type}\",\n",
    "                        cluster_phi_domain[domain_name],\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#pre_checkpoint_dir = check_pretrain_dir(config_json)\n",
    "AgentClass = Agent #globals()\n",
    "agent = AgentClass(config)\n",
    "\n",
    "\n",
    "if pre_checkpoint_dir is not None:\n",
    "    agent.load_checkpoint(\"model_best.pth.tar\", pre_checkpoint_dir)\n",
    "    \n",
    "try:\n",
    "    agent.run()\n",
    "    #agent.finalise()\n",
    "except KeyboardInterrupt:\n",
    "    pass "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fuda",
   "language": "python",
   "name": "fuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
